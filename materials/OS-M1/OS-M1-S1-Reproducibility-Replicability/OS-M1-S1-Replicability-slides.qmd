---
title: "Threats to credible research: How Open Research can change the game"
author: "Sarah von Grebmer zu Wolfsthurn"
date: "today"
date-format: "DD/MM/YYYY"
format: 
  revealjs:
    css: ../../../slides-custom.css # looks for css file in root
    footer: LMU Open Science Center
    slide-number: true
    logo: ../../../OSC_FORRT_Logo.png  # Inserts logo in the bottom right corner (default)
  pdf:
    geometry: 
      - top=30mm
      - left=30mm
  pptx: 
    css: ../../../slides-custom.css # looks for css file in root

bibliography: ../../../assets/references.bib
csl: ../../../assets/apa.csl

execute:
  echo: true
  eval: true
  engine: knitr
---

## Licence

<br>

<p style="text-align:center;">
  <img src="https://licensebuttons.net/l/by/4.0/88x31.png"
       alt="CC BY 4.0"
       style="height:50px;">
</p>

<div style="background-color: #f0f0f0; padding: 0.05em; border-radius: 2px; font-size: 0.6em;">
This work was originally created by [Felix Schoenbrodt](https://www.nicebread.de/) under a CC-BY 4.0 [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en). This current work by Sarah von Grebmer zu Wolfsthurn, Malika Ihle and Felix Schoenbrodt is licensed under a CC-BY 4.0 [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en). It permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
</div>

::: {.notes}
**Speaker Notes**: The Creative Commons Attribution 4.0 International (CC BY 4.0) license permits you to freely use, share, and adapt the licensed material for any purpose, including commercial use. This means you can copy, redistribute, remix, transform, and build upon the work without asking for permission.

However, there are some important conditions you must follow. You are required to give appropriate credit to the original creator, provide a link to the license, and indicate if you made any changes to the material. Additionally, you cannot apply any legal terms or technological measures that would restrict others from using the work under the same freedoms.
:::

---

## Contribution statement

<br>

**Creator**: Von Grebmer zu Wolfsthurn, Sarah (![ORCID Logo](https://orcid.org/sites/default/files/images/orcid_16x16.png)[ 0000-0002-6413-3895](https://orcid.org/0000-0002-6413-3895))

**Reviewer**: Ihle, Malika (![ORCID Logo](https://orcid.org/sites/default/files/images/orcid_16x16.png)[0000-0002-3242-5981](https://orcid.org/0000-0002-3242-5981))

**Consultant**: Schönbrodt, Felix (![ORCID Logo](https://orcid.org/sites/default/files/images/orcid_16x16.png)[0000-0002-8282-3910](https://orcid.org/0000-0002-8282-3910))

::: {.notes}
**Speaker Notes**: These are the **speaker notes**. You will a script for the presenter for every slide. In presentation mode, your audience will not be able to see these speaker notes, they are only visible to the presenter. 

**Instructor Notes**: There are also **instructor notes**. For some slides, there will be pedagogical tips, suggestons for acitivities and troubleshooting tips for issues your audience might run into. You can find these notes underneath the speaker notes.

**Acessibility Tips**: Where applicable, this is a space to add any tips you may have to facilitate the accessibility of your slides and activities. 
:::

---

## Prerequisites

::: {.callout-important}
Before completing this submodule, please carefully read about the prerequisites.
:::

| Prerequisite   |  Description  | Link/Where to find it   |
|------------|------------|------------|
| UNESCO Recommendations on Open Science | Recommended reading pages 6-19 | [Download Link](https://unesdoc.unesco.org/ark:/48223/pf0000379949) |

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: These are the prerequisites for this submodule. Before you get started on this submodule with your audience, you need to ensure that the audience fulfills these criteria. Outline any essential prerequisites (software, tools, other submodules etc.) here in table format. If you prefer bullet points to list the prerequisites, delete the table and use bullet points instead. 
:::

---

## Before we start - survey time



::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: The pre-submodule survey serves to examine students' prior knowledge about the submodule's topic.
- Use free survey software such as particify or formR to establish the following questions (shown on separate slides).
:::

---

**Based on your experience so far, how would you currently rate your trust in published scientific findings on a scale from 1 - 5? (1 = not trusting any of the findings, 2 = trusting only some findings, 3 = trusting about half of the findings, 4 = trusting the majority of the findings,  5 =  trusting all findings)**

a. 1

b. 2

c. 3

d. 4

e. 5

---

**Based on your experience so far, do you currently see any challenges in research?**

Wordcloud answer. 

---

**Based on your experience so far, which concepts to you connect to research more broadly?**

Wordcloud answer. 

---


---

**What is your level of familiarity with Open Research practices in general (e.g., basic concepts, terminology, or tools)?**

a. I am unfamiliar with the concept of Open Research practices.

b. I have heard of them but I would not know how they apply to my work.

c. I have basic understanding and experience with Open Research practices in my own work/research/studies. 

d. I am very familiar with Open Research practices and routinely apply them in my daily work/research/study routines.

---


## Discussion of survey results

<br>

<div style="background-color: #f0f0f0; padding: 0.1em; border-radius: 5px; font-size: 1em; text-align: center;">

What do we see in the results?

</div>

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim"**: Briefly examine the answers given to each question interactively with the group.
- Use visuals from the survey to highlight specific answers.
:::

---

## Where are we at?

**Previously**:

- Point 1
- Point 2

<div style="background-color: #f0f0f0;">
**Up next**:

- Point 1
- Point 2
</div>


::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: Place the topic of the current submodule within a broader context.
- Remind students what you are working towards and what the bigger picture is.
:::

---

## Covered in this session

- **Aim**: This slides serves as an overview of the topics that are discussed, presented as bullet point:
- Topic 1
- Topic 2
- Topic 3

::: {.notes}
**Speaker Notes**: Script for the slide here.
**Instructor Notes**: Add.
:::

---

## Learning goals

- **Aim**: Formulate specific, action-oriented goals learning goals which are measurable and observable in line with Bloom's taxonomy (Anderson et al., 2001; Bloom et al., 1956)

- Place an emphasis on the **verbs** of the learning goals and choose verbs that align with the skills you want to develop or assess.
- Examples: 
  - Students will **describe** the process of photosynthesis or
  - Students will **construct** a diagram illustrating the process of photosynthesis
  
::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: Formulate specific, action-oriented goals learning goals which are measurable and observable in line with Bloom's taxonomy (Anderson et al., 2001; Bloom et al., 1956)
- Place an emphasis on the **verbs** of the learning goals and choose verbs that align with the skills you want to develop or assess.
- Examples: 
  - Students will **describe** the process of photosynthesis or
  - Students will **construct** a diagram illustrating the process of photosynthesis

:::

---

## Key terms and definitions

- **Aim**: Introduce key terms and definitions that students will come across throughout the session.
<br>

- **Key Term 1**: Definition
- **Key Term 2**: Definition
- **Key Term 3**: Definition

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Base yourself on conceptual change theory and examine existing concepts in relation to some key terms. Re-examine the formation of new concepts at the end of the lesson. 
:::

---

## Practical Exercise 1

**What does "research" even mean?**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Use the think-pair-share method: Students write down for themselves for a minute what a definition of research could be, then pair with a neighbour to compare answers. Final answers are then discussed in the plenum. 
:::

---

## What does "research" even mean?

"Research refers to a careful, well-defined (or redefined), objective, and **systematic method** of **search for knowledge**, or **formulation** of a theory that is driven by inquisitiveness for that which is unknown and useful on a particular aspect so as to make an original contribution to expand the existing knowledge base. Research involves the **formulation of hypothesis** or **proposition of solutions**, data analysis, and deductions; and ascertaining whether the conclusions fit the hypothesis. Research is a **process of creating, or formulating knowledge that does not yet exist.**"

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Deb, D., Dey, R., & Balas, V. E. (2018). Introduction: What is research?. In *Engineering Research Methodology: A Practical Insight for Researchers* (pp. 1-7). Singapore: Springer Singapore.
</div>

---

## The research cycle

<img src="images/00_scientific_method.png" alt="A  diagram illustrating the scientific, with six stages arranged clockwise: Observation/Question, Research topic area, Hypothesis, Test with experiment, Analyze data, and Report Conclusions connected by arrows to show the ongoing, iterative research process." style="display:block; margin:0 auto; width:100%; height:100%;">


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
[www.phdcomics.com](www.phdcomics.com)
</div>


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---


## The research cycle

<img src="images/00_research_cycle.png" alt="A circular diagram illustrating the design-based research cycle, with four stages arranged horizontally: Observation, formulation hypothesis, test hypothesis with experiment, establish theory based on repeated validation of results connected by arrows to show the ongoing, iterative research process." style="display:block; margin:0 auto; width:50%; height:50%;">


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
DBR English greyscaler (Design-based research cycle)” by Sarah Zloklikovits, licensed under CC BY 4.0 — Wikimedia Commons.
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

# Can we trust in research?

---

## What is "trust" in research (general public perspective)?

::: incremental
- "*Society trusts that scientific research results are an honest and accurate reflection of a researcher’s work.*" <div style="display: flex; font-size: 0.55em; color: #777;">(Committee on Science, Engineering and Public Policy 2009: ix)</div>


- "*The public must be able to trust the science and scientific process informing public policy decisions.*" <div style="display: flex; font-size: 0.55em; color: #777;">(Obama 2009)</div>


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Resnik, D. B. (2011). Scientific research and the public trust. *Science and engineering ethics, 17*(3), 399-409.
</div>
:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

## What is "trust" in research?

::: incremental
- No consensus on definition from the perspective fo researchers but:
  - Trust is essential for **effective collaboration** among researchers (includes co-authorship, peer review, data sharing, replication, teaching, mentoring etc.)
  - Authors submitting papers or grant proposals rely on reviewers to evaluate their work competently, fairly, and confidentially
  - Scientists reading published research trust that the **work was conducted as described**, that **all relevant methodological details** are **disclosed**, and that the data have *not been fabricated or falsified*


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Resnik, D. B. (2011). Scientific research and the public trust. *Science and engineering ethics, 17*(3), 399-409.
</div>

:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

## Replicability and reproducibility


<div style="font-size: 0.8em;">
| Feature | **Replicability**| **Reproducibility** |
|------------|------------------|---------------------|
| Definition | Ability to **repeat an experiment** using the **same methods** and obtain the same results | Ability to **obtain consistent results** using the **original data and code** |
| Focus                 | *aka* repeating the experiment in practice  | *aka* re-analyzing the original data|
| Materials | Same experiment setup, protocols, conditions etc. | Original data, analysis scripts, code etc.|
| Example  | Running the same psychological experiment with new participants | Running the published statistical analysis on the shared dataset |
</div>



---

## Practical exercise 2

Decide whether each scenario in the following slides is an example of **reproducibility** or **replicability**. 

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Think pair share. 
:::

---

## Scenario 1

<br>
<br>

**A computational neuroscientist reruns a published fMRI analysis using the original dataset and Python scripts to verify the reported brain activation patterns.**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Reproducibility
:::

---

## Scenario 2

<br>
<br>

**An environmental scientist repeats a field experiment on soil nutrient levels using the same sampling protocol at a different site.**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Replicability
:::

---

## Scenario 3

<br>
<br>

**A linguist reanalyzes a corpus of historical texts using the same annotation guidelines and code to verify reported patterns of syntactic structures.**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Reproducibility
:::

---

## Scenario 4

<br>
<br>

**A psychology lab replicates a social behavior experiment using new participants from a different cultural background.**


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Replicability
:::

---

# Interactive slide

---

<section>
  <h2 style="font-size:1.5em;">Practical Exercise 1</h2>
  <p style="font-size:0.85em;">Decide whether each scenario is an example of <strong>reproducibility</strong> or <strong>replicability</strong>.</p>
  <ol style="font-size:0.8em;">
    <li class="fragment">
      A computational neuroscientist reruns a published fMRI analysis using the original dataset and Python scripts to verify the reported brain activation patterns.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Reproducibility</div>
    </li>
    <li class="fragment">
      An environmental scientist repeats a field experiment on soil nutrient levels using the same sampling protocol at a different site.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Replicability</div>
    </li>
    <li class="fragment">
      A psychology lab replicates a social behavior experiment using new participants from a different cultural background.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Replicability</div>
    </li>
    <li class="fragment">
      A linguist reanalyzes a corpus of historical texts using the same annotation guidelines and code to verify reported patterns of syntactic structures.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Reproducibility</div>
    </li>
  </ol>
</section>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Replicability
:::

---

## What published research can be replicated/reproduced? REMOVE

<div style="font-size: 0.5em;">
| Field                          | Success | Failure |
|--------------------------------|---------|---------|
| OSC (2015) – Psychology         | 36%     | 64%     |
| Chang & Li (2015) – Economics (67 papers, 29 papers replicated)        | 43%     | 57%     |
| Camerer 2016 – Econ laboratory        | 61%     | 39%     |
| Camerer combined Social Sci     | 62%     | 38%     |
| Begley & Ellis (2012) – Cancer Research | 11%     | 89%     |
| Prinz et al. (2011) – Pharmaceutical research      | 35%     | 65%     |
| Cova et al. (2018) – x-philosophy       | 70%     | 30%     |
| Protzko et al. (2023) – Social   | 86%     | 14%     |
</div>


---

## Replicability and reproducibility across disciplines

```{r, echo = F}
library(ggplot2)
library(tidyverse)

# Sample data
df <- data.frame(Category = rep(c("Psychology (n = 97)", 
                                  "Cancer Research (n = 53)", 
                                  "Pharmaceutical Research (n = 67)",
                                  "Economics (n = 67)", 
                                  "Experimental Economics (n = 16)",
                                  "Experimental Philosophy (n = 40)"), 
                                each = 2),
                 Result = rep(c("successfully replicated",
                                "unsuccessfully replicated"), 
                              times = 6),
  Value = c(36, 64, 11, 89, 35, 65, 43, 57, 61, 39, 70, 30)
)

# Preserve order of bars on x-axis
df$Category <- factor(
  df$Category,
  levels = c("Psychology (n = 97)", 
             "Cancer Research (n = 53)", 
             "Pharmaceutical Research (n = 67)",
             "Economics (n = 67)", 
             "Experimental Economics (n = 16)",
             "Experimental Philosophy (n = 40)")
)

# Make stacking order explicit by setting factor levels for Series.
df$Series <- factor(df$Result, 
                    levels = c("successfully replicated",
                               "unsuccessfully replicated"))

# Plot
ggplot(df, aes(x = Category, y = Value, fill = Result)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(Value, "%")), # use position_stack(vjust = 0.5) to center a label inside each segment
            position = position_stack(vjust = 0.5),
            color = "white",
            size = 5) +
  labs(title = "",
       x = "",
       y = "Percentage (%)") +
  scale_fill_manual(values = c("successfully replicated" = "darkblue",
                               "unsuccessfully replicated" = "darkred"),
                    labels = function(x) str_wrap(x, width = 12)) + # wrap legend names
  theme_minimal() +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) + # to wrap text
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),
        axis.text.y = element_text(size = 14),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        legend.text = element_text(size = 14))


```


<div style="font-size: 0.3em; color: #777;">
Begley, C. G., & Ellis, L. M. (2012); Camerer et al (2016); Chang & Li (2015); Cova et al. (2018); Open Science Collaboration (2015); Social Science: Combined sample of systematically sampled projects (RPP, SSRP, EERP); Prinz, F., Schlange, T., & Asadullah, K. (2011); Protzko et al. (2023)
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## Psychology: The Reproducibility Project (2015)

- Large-scale replication project:
  - Close/exact replications of 100 experimental and correlational studies from 3 different psychological journals
  - Reproducibility evaluated based on effect sizes, p-values, subjective assessment of replication teams
  - Contacted original study authors when necessary

::: {.callout-important}
**What did they find?**

*Large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors.*
:::

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. *Science, 349*(6251), aac4716.
</div>

::: {.notes}
**Speaker Notes**: The Reproducibility Project by the Open Science Collaboration (2015) involved a large-scale, collaborative effort by over 270 researchers. They selected 100 experimental studies published between 2008 and 2012 in three top psychology journals: Psychological Science, Journal of Personality and Social Psychology, and Journal of Experimental Psychology: Learning, Memory, and Cognition.
For each study, they carefully reviewed the original materials, methods, and analyses, and often contacted the original authors for clarification or access to materials and protocols. Each replication aimed to match the original study’s sample size, experimental design, and analysis plan as closely as possible, while making only minimal changes when exact materials were unavailable. The replications were preregistered to specify hypotheses, design, and analysis before data collection, reducing bias and “researcher degrees of freedom." Data collection was conducted independently from the original research teams, often in multiple labs to ensure rigor. After data collection, they analyzed the results using the same statistical methods as the original studies to determine whether the findings could be reproduced. The project also evaluated effect sizes, not just statistical significance, to compare the magnitude of the original and replicated effects. **Only 39% of the replications produced statistically significant results, and the median effect size in replications was about half that of the original studies, highlighting the reproducibility challenges in psychology**.

**Instructor  Notes**:
:::

---

## Psychology: The Replication Database by FORRT

- Collects replication results across different psychological fields
- Provide detailed overview of the original findings and the replication outcomes

![](images/FORRT_Replication_Database.png){fig-align=center width=100% fig-alt="An image of the landing page of the FORRT Replication database"}


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
**FORRT Replication Database** ([https://forrt-replications.shinyapps.io/fred_explorer/](https://forrt-replications.shinyapps.io/fred_explorer/))
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

## What does this mean for research? 

- Published findings across fields to be viewed with **caution**?
- "We know" --> "We *think* we know"?
- More research to verify existing "truths"?

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

## Not a recent issue

<img src="images/reproducibility_literature.png" alt="An image with te titles of journal articles reporting about the replicability and reproducibility crisis across difference disciplines." style="display:block; margin:0 auto; width:100%; height:100%;">


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Adapted from Dr. Malika Ihle: [https: https://osf.io/u3znx](https://osf.io/u3znx)
</div>


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## A romanticised idea of research?

<img src="images/this-is-research.png" alt="An image describing a romantic view of research: The smartest heads in the world immerse themselves into a research topic for years. In that process, they become the experts, nobody knows more about that topic.The boundaries of knowledge have been pushed forward. When the researchers are confident in their findings, they publish them in the best scientific journals, with the highest standards of quality, rigor, and integrity." style="display:block; margin:0 auto; width:100%; height:100%;">

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

# Why did not more studies replicate?

---

## Why published papers are useful

::: incremental
- Getting a **job**

- Being awarded **grants**

- Being **visible** in the respective research field

- ...
:::

---

## Why published papers are useful

- Getting a **job**

- Being awarded **grants**

- Being **visible** in the respective research field

- ...

::: {.callout-warning}
The consequence can be a "*rat race*" culture: Researchers try to publish as much as they can (Schmidt et al., 2021)
:::


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## Career relevance of publishing

- Survey among N = 1453 psychology researchers, 66% were actually members of a professorship hiring committee

<div style="font-size: 0.7em;">
| **Actual (not desired) relevance in professorship hiring committees** | **Rank** |
|------------------------------------------------------------------|------|
| **Number** of peer-reviewed publications | 1 |
| Fit of research profile to the hiring department | 2 |
| Quality of research talks | 3 |
| **Number** of publications | 4 |
| Volume of acquired third party funding | 5 |
| **Number** of first authorships | 6 |
</div>


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Abele-Brehm, A. E., & Bühner, M. (2016). Wer soll die Professur bekommen? *Psychologische Rundschau, 67*(4), 250–261. [http://doi.org/10.1026/0033-3042/a000335](http://doi.org/10.1026/0033-3042/a000335)
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## How do I get lots of publications?

![](images/02_significant_results_discipline_92.png){fig-align=center width=100% fig-alt="significant results"}

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Fanelli, D. (2010). “Positive” Results Increase Down the Hierarchy of the Sciences. *PLOS ONE, 5*, e10068. [https://doi.org/10.1371/journal.pone.0010068](https://doi.org/10.1371/journal.pone.0010068)
</div>

---

## Publication bias

"If my study *works*, I can publish it. *If it does not, let's hide it the drawer.*"

```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/file-drawer.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">
**Publication bias** happens when studies with *positive* or *significant* results are much more likely to be published than studies with negative or non-significant results.
  </div>

</div>

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Song, F., Hooper, L., & Loke, Y. K. (2013). Publication bias: what is it? How do we measure it? How do we avoid it?. *Open Access Journal of Clinical Trials*, 71-81. [https://doi.org/10.2147/OAJCT.S34419](https://doi.org/10.2147/OAJCT.S34419)
</div>

---

## Example publication bias

![](images/Turner_Study_Publication_Bias.png){fig-align=center width=100% fig-alt="Turner study"}

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Turner, E. H., Matthews, A. M., Linardatos, E., Tell, R. A., & Rosenthal, R. (2008). Selective publication of antidepressant trials and its influence on apparent efficacy. *New England Journal of Medicine, 358*(3), 252-260. [https://doi.org/10.1056/NEJMsa065779](https://doi.org/10.1056/NEJMsa065779)
</div>

::: {.notes}
**Speaker Notes**: In this study, the authors obtained the archive of clinical trials for 12 antidepressant drugs submitted the the US Food and Drug Administration (FDA). In total, the trials involved more than 12000 patients. They performed a systematic search to identify which of these trials had been published in the literature. For those that were published, the compared the published versions and results with the versions and results in the FDA documentation. There were 74 FDA-registered trials. Of those, 38 trials showed postive effects of antidepressant drugs, 36 showed negative effects of antidepressant drugs. This is a 51% - 49% split of results: about half of the FDA-registered trials showed a positive effect of antidepressant drugs, the other half showed a negative effect of antidepressant drugs. 
37 of the 38 trials that showed positive effects were published in the literature, basically all of them except 1 trial. However, of the 36 trials that showed negative or no effects of antidepressant drugs, only 3 were published that included those exact findings. 22 of the 36 trials were not published at all, and the remaining 11 trials were published with the outcomes reframed in a more positive way. In the literature, in the end, we ended up with a total of 51 trials being published, 48 of which showed positive effects of antidepressant drugs on depression. That is 94% of this sample of papers. Only 6% (3 papers) showed a negative effect of antidepressant drugs on depression. As a consequence, those of us searching the literature for these papers, will get a distorted via of the effectiveness of antidepressants. Therefore, the authors were able to show a substantial publication bias in the antidepressant trial literature. 
**Instructor  Notes**: Add. 
:::


---

## More than just publication bias

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
de Vries, Y. A., Roest, A. M., de Jonge, P., Cuijpers, P., Munafò, M. R., & Bastiaansen, J. A. (2018). The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: the case of depression. *Psychological Medicine, 48*(15), 2453–2455. [https://doi.org/10.1017/S0033291718001873](https://doi.org/10.1017/S0033291718001873)
</div>

::: incremental

```{=html}

<div style="display: flex; align-items: flex-start; gap: 0.5em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/12_Publication-bias.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1; font-size:0.8em;">

  (a) All conducted studies
  (b) **Study publication bias**: non-publication of an entire study
  (c) **Outcome reporting bias**: non-publication of negative outcomes within a published article or switching the status of (non-significant) primary and (significant) secondary outcomes
  (d) **Spin**: authors conclude that the treatment is effective despite non-significant results on the primary outcome
  (e) **Citation bias**: Studies with positive results receive more citations than negative studies


  </div>

</div>

:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## "Thought so already" bias

![](images/14_cognitive_bias.png){fig-align=center width=100% fig-alt="confirming your own beliefs"}

---

## "I knew it" bias

![](images/hindsight-bias.png){fig-align=center width=700% fig-alt="confirming your own beliefs"}


---

## Practical exercise 2

**Task**: Match the bias to its description.

<div style="font-size:0.6em;">

| **Bias** | **Description** |
|------------|-----------------|
| 1. Confirmation bias | A. Non-publication of negative outcomes within a paper, or switching non-significant primary outcomes with significant secondary ones.  |
| 2. Spin | B. Studies with positive results receive more citations than negative studies.  |
| 3. Study publication bias | C. After learning the outcome, believing “I knew it all along.” |
| 4. Hindsight bias | D. Non-publication of an entire study (e.g., trials with null results never submitted). |
| 5. Outcome reporting bias | E. Tendency to seek or interpret information in ways that confirm existing beliefs. |
| 6. Citation bias | F. Authors conclude the treatment is effective despite non-significant primary outcomes.|

</div>


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Solutions: 1E, 2F, 3D, 4C, 5A, 6B.  
:::

---

## Accidental p-hacking

**"P-hack... What now?"**

![](images/04_QRPs.png){fig-align=center width=100% fig-alt="p-hacking and questionble research practises"}

---

## Excursus: Null-Hypothesis-Significance Testing

**Example**: Does this new drug work better to decrease flu-symptoms compared to an existing drug?

::: incremental
- H₀ = “the new drug is not better than the existing one.”
- Collect data from experiments and perform a statistical analysis to see if the evidence is strong enough to reject H₀
- **P-value** = how likely it is to see the results you got **if H₀ is true**.
- A p-value of 0.05 means: “There’s a 5% chance of seeing these results (or more extreme) if the null hypothesis is true.”
:::

::: {.callout-important}
A *p*-value of 0.05 means that we accept a 5% chance that our results came about by pure luck (if the results were accidental, we would speak of a false positive result)
:::

---

## Increasing your false positive rates can happen in the blink of an eye!

::: {.callout-caution}
Do **not** try the following at home.
:::

---

# Continue from here


---

## Have you ever ...?

- Neglected to blind data collection?
- Added more outcome variables in the analysis process and reported only the outcome variables that produced a significant effect?
- Ran many different comparisons on different subsets/groups etc.?
- Stopped data collection when you obtained the result you were looking for?
- Continued sampling after finding a null result?
- Reformulated your hypotheses based on what you found?
- Excluded outliers based on the significance of your results
- Tested excluding, including, or transforming covariates, but only reported the final model?
- Neglected to report all the dependent measures you tested?

---

## Hack 1: Adding lots of outcome variables

- For **two outcome variables**: False positive rate increases from 5% to **9.5%**

- For **five outcome variables**: False positive rate increases from 5% to **41%**


![](images/05_Outcome_Switching.png){fig-align=center width=100% fig-alt="p-hacking and questionble research practises"}

---

## Hack 2: Run as many comparisons as possible


```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/06_Reporting_Conditions.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">

  - Run as many different comparisons on different outcomes, subgroups, time windows etc. as you can
  - Only report the comparisons that produced a statistically significant result



  </div>

</div>

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
ADD REFERENCE
</div>

---

## Hack 3: Stop collecting data whenever you found what you were looking for


```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/15_optional_stopping.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">

  - Collect an initial sample, analyze the results, add participants if the results are not significant
  - Stop when significance is found
      - One analysis: α = 5%
      - Two analyses: α = 11%
      - But with enough “just looking”  can be pushed to 100%!



  </div>

</div>

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Armitage, P., McPherson, C. K., & Rowe, B. C. (1969). Repeated significance tests on accumulating data. *Journal of the Royal Statistical Society. Series A (General), 132*, 235–244.
</div>

---

## Hack 4: Drop participants you do not like


```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/08_Selective_Exclusion.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">

  - Selectively exclude data/ outliers after seeing the results until the results are satisfying (aka until significance has been reached)




  </div>

</div>

---

# Continze from here

---


## Tip 2: Experiment with many conditions, but report only those that worked


```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/06_Reporting_Conditions.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">

  - Transforming a thesis into a ground-breaking publication
  - The „Chrysalis effect“ (O’Boyle et al, 2017): 142 dissertations that were subsequently published in a refereed journal (149 field studies and 26 experiments)


  </div>

</div>



O’Boyle, E. H., Banks, G. C., & Gonzalez-Mulé, E. (2017). The Chrysalis Effect: How Ugly Initial Results Metamorphosize Into Beautiful Articles. Journal of Management, 43(2), 376–399. https://doi.org/10.1177/0149206314527133
https://twitter.com/JoeHilgard/status/699693258386051072

---

## Tip 3: Stop collecting data whenever you feel like it

- Collect an initial sample, analyze the results, add additional participants if not significant, stop when significance is found

- Increase twice: α = 11%

- But with enough looks can be pushed to 100%!



---

## Tip 4: Drop participants that you did not like

- Selective exclusion of data/outliers after seeing the results until the results are satisfying (aka until significance has been reached)

![](images/08_Selective_Exclusion.png){fig-align=center width=50% fig-alt="selective exclusion"}

---

## Tip 5: HARK-ing (personal favourite)

- **H**ypothesizing **A**fter **R**esults are **K**nown = presenting an exploratory finding to match a hypothesis that was created only after analysing the results

---

## Tip 6: Run as many different comparisons as humanly possible

- Run many different comparisons on different outcomes, subgroups, time windows, etc., and only report the comparison that produced p < 0.05

SPEAKER NOTES: Each time you try another independent test at α = 0.05 you have a 5% chance of a Type I error for that test. If you perform many uncorrected tests, the chance that at least one will be (falsely) significant rises quickly. For example, doing 20 independent tests at α = 0.05 gives a probability of at least one false positive of
1 − (1 − 0.05)²⁰ = 1 − 0.95²⁰ ≈ 0.6415 (≈64%).



---



## Human errors


---

## Statistical errors


---

## AI and the promotion of untrustworthy research 


---

## Quality, rigor and integrity

*The smartest heads in the world immerse themselves into a research topic for years. In that process, they become the experts – nobody knows more about that topic. 
The boundaries of knowledge have been pushed forward. When the scientist are confident in their findings, they publish them in the best scientific journals, with the highest standards of quality, rigor, and integrity.*

::: {.callout-important}
How much of that literature is true do you think?
:::


::: {.notes}
**Speaker Notes**: Script for the slide here. 

**Instructor Notes**: Add. 
:::

---

## Why should we trust researchers?

![](images/01_snakesman_researcher.png){fig-align=center width=100% fig-alt="snakesman analogy"}

::: {.notes}
**Speaker notes**: Add script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: Core theoretical introduction of submodule topic.
- For a 90-minute lesson, the instructor should try to "lecture" for only 20 minutes, students should work in groups/pairs/on their own for at least 55 minutes of the lesson (+ a 15 minute break).
- Pair theoretical aspects with practical exercises and group discussions according to the Think-Pair-Share style and according to Cognitive Load Theory (Sweller, 1980).
- Use multiple slides for this part.
:::

---

## What research is all about

Text

---

# How to be successful in academia (or: How to hack your way to scientific glory)


---

## Tips on how to p-hack (like the pros)

---

## Refs

Schmidt, R., Curry, S., & Hatch, A. (2021). Creating SPACE to evolve academic assessment. Elife, 10, e70929.

---

## ...right? MOVE

![](images/07_holy_grail.png){fig-align=center width=50% fig-alt="chasing significance"}

---

## Decide where to add

**Ideal scenario: Balancing the desire to stay truthful to research with the necessity to publish?**


---