---
title: "Replication failure, reproducibility crisis and other threats to credible science: How Open Research can change the game"
author: "LMU Open Science Center"
date: "today"
date-format: "DD/MM/YYYY"
format: 
  revealjs:
    css: ../../../slides-custom.css # looks for css file in root
    footer: LMU Open Science Center
    slide-number: true
    logo: ../../../OSC_FORRT_Logo.png  # Inserts logo in the bottom right corner (default)
  pdf:
    geometry: 
      - top=30mm
      - left=30mm
  pptx: 
    css: ../../../slides-custom.css # looks for css file in root

bibliography: ../../../assets/references.bib
csl: ../../../assets/apa.csl

execute:
  echo: true
  eval: true
  engine: knitr
---

## Licence

<br>

<p style="text-align:center;">
  <img src="https://licensebuttons.net/l/by/4.0/88x31.png"
       alt="CC BY 4.0"
       style="height:50px;">
</p>

<div style="background-color: #f0f0f0; padding: 0.05em; border-radius: 2px; font-size: 0.6em;">
This work was originally created by [Felix Schoenbrodt](https://www.nicebread.de/) under a CC-BY 4.0 [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en). This current work by Sarah von Grebmer zu Wolfsthurn, Malika Ihle and Felix Schoenbrodt is licensed under a CC-BY 4.0 [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en).
</div>

::: {.notes}
**Speaker Notes**: The Creative Commons Attribution 4.0 International (CC BY 4.0) license permits you to freely use, share, and adapt the licensed material for any purpose, including commercial use. This means you can copy, redistribute, remix, transform, and build upon the work without asking for permission.

However, there are some important conditions you must follow. You are required to give appropriate credit to the original creator, provide a link to the license, and indicate if you made any changes to the material. Additionally, you cannot apply any legal terms or technological measures that would restrict others from using the work under the same freedoms.
:::

---

## Contribution statement

<br>

**Creator**: Von Grebmer zu Wolfsthurn, Sarah (![ORCID Logo](https://orcid.org/sites/default/files/images/orcid_16x16.png)[ 0000-0002-6413-3895](https://orcid.org/0000-0002-6413-3895))

**Reviewer**: Ihle, Malika (![ORCID Logo](https://orcid.org/sites/default/files/images/orcid_16x16.png)[0000-0002-3242-5981](https://orcid.org/0000-0002-3242-5981))

**Consultant**: Schönbrodt, Felix (![ORCID Logo](https://orcid.org/sites/default/files/images/orcid_16x16.png)[0000-0002-8282-3910](https://orcid.org/0000-0002-8282-3910))

::: {.notes}
**Speaker Notes**: These are the **speaker notes**. You will a script for the presenter for every slide. In presentation mode, your audience will not be able to see these speaker notes, they are only visible to the presenter. 

**Instructor Notes**: There are also **instructor notes**. For some slides, there will be pedagogical tips, suggestons for acitivities and troubleshooting tips for issues your audience might run into. You can find these notes underneath the speaker notes.

**Acessibility Tips**: Where applicable, this is a space to add any tips you may have to facilitate the accessibility of your slides and activities. 
:::

---

## Prerequisites

::: {.callout-important}
Before completing this submodule, please carefully read about the prerequisites.
:::

| Prerequisite   |  Description  | Link/Where to find it   |
|------------|------------|------------|
| UNESCO Recommendations on Open Science | Recommended reading pages 6-19 | [Download Link](https://unesdoc.unesco.org/ark:/48223/pf0000379949) |

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: These are the prerequisites for this submodule. Before you get started on this submodule with your audience, you need to ensure that the audience fulfills these criteria. Outline any essential prerequisites (software, tools, other submodules etc.) here in table format. If you prefer bullet points to list the prerequisites, delete the table and use bullet points instead. 
:::

---

## Before we start - survey time



::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: The pre-submodule survey serves to examine students' prior knowledge about the submodule's topic.
- Use free survey software such as particify or formR to establish the following questions (shown on separate slides).
:::

---

**What is your level of familiarity with Open Research practices in general (e.g., basic concepts, terminology, or tools)?**

a. I am unfamiliar with the concept of Open Research practices.

b. I have heard of them but I would not know how they applies to my work.

c. I have basic understanding and experience with Open Research practices in my own work. 

d. I am very familiar with Open Research practises and routinely apply them in my daily research routines.

---

**In your research, when do you think about research quality and integrity?**

a. I do not think these principles are relevant for my research. 

b. In the planning process

c. Concept 3

d. Throughout the entire research process

---

**On a scale from 1 to 5, how trusting are you of published results in the field of psychology? (1 = not trusting any of the results, 2 = trusting only some results, 3 = trusting about half of the results, 4 = trusting the majority of the results,  5 =  trusting all results)**

a. 1

b. 2

c. 3

d. 4

e. 5

---

## Discussion of survey results

<br>

<div style="background-color: #f0f0f0; padding: 0.1em; border-radius: 5px; font-size: 1em; text-align: center;">

What do we see in the results?

</div>

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim"**: Briefly examine the answers given to each question interactively with the group.
- Use visuals from the survey to highlight specific answers.
:::

---

## Where are we at?

**Previously**:

- Point 1
- Point 2

<div style="background-color: #f0f0f0;">
**Up next**:

- Point 1
- Point 2
</div>


::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: Place the topic of the current submodule within a broader context.
- Remind students what you are working towards and what the bigger picture is.
:::

---

## Covered in this session

- **Aim**: This slides serves as an overview of the topics that are discussed, presented as bullet point:
- Topic 1
- Topic 2
- Topic 3

::: {.notes}
**Speaker Notes**: Script for the slide here. 
:::

---

## Learning goals

- **Aim**: Formulate specific, action-oriented goals learning goals which are measurable and observable in line with Bloom's taxonomy (Anderson et al., 2001; Bloom et al., 1956)

- Place an emphasis on the **verbs** of the learning goals and choose verbs that align with the skills you want to develop or assess.
- Examples: 
  - Students will **describe** the process of photosynthesis or
  - Students will **construct** a diagram illustrating the process of photosynthesis
  
::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: Formulate specific, action-oriented goals learning goals which are measurable and observable in line with Bloom's taxonomy (Anderson et al., 2001; Bloom et al., 1956)
- Place an emphasis on the **verbs** of the learning goals and choose verbs that align with the skills you want to develop or assess.
- Examples: 
  - Students will **describe** the process of photosynthesis or
  - Students will **construct** a diagram illustrating the process of photosynthesis
  
:::

---

## Key terms and definitions

- **Aim**: Introduce key terms and definitions that students will come across throughout the session.
<br>

- **Key Term 1**: Definition
- **Key Term 2**: Definition
- **Key Term 3**: Definition

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Base yourself on conceptual change theory and examine existing concepts in relation to some key terms. Re-examine the formation of new concepts at the end of the lesson. 
:::

---


# The research process


---

## The research cycle

<img src="images/00_scientific_method.png" alt="A  diagram illustrating the scientific, with six stages arranged clockwise: Observation/Question, Research topic area, Hypothesis, Test with experiment, Analyze data, and Report Conclusions connected by arrows to show the ongoing, iterative research process." style="display:block; margin:0 auto; width:50%; height:50%;">


<div style="font-size: 0.55em; color: #777;">
[www.phdcomics.com](www.phdcomics.com)
</div>


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---


## The research cycle

<img src="images/00_research_cycle.png" alt="A circular diagram illustrating the design-based research cycle, with four stages arranged horizontally: Observation, formulation hypothesis, test hypothesis with experiment, establish theory based on repeated validation of results connected by arrows to show the ongoing, iterative research process." style="display:block; margin:0 auto; width:50%; height:50%;">


<div style="font-size: 0.55em; color: #777;">
DBR English greyscaler (Design-based research cycle)” by Sarah Zloklikovits, licensed under CC BY 4.0 — Wikimedia Commons.
</div>

---

# Can we trust research?

---

## Replicability and reproducibility

| Feature | **Replicability**| **Reproducibility** |
|------------|------------------|---------------------|
| Definition | Ability to **repeat an experiment** using the **same methods** and obtain the same results | Ability to **obtain consistent results** using the **original data and code** |
| Focus                 | *aka* repeating the experiment in practice  | *aka* re-analyzing the original data|
| Materials | Same experiment setup, protocols, conditions etc. | Original data, analysis scripts, code etc.|
| Example  | Running the same psychological experiment with new participants | Running the published statistical analysis on the shared dataset |



---

## Practical exercise 1

Decide whether each scenario in the following slides is an example of **reproducibility** or **replicability**. Write down your answers. 

instructor notes: think pair share

---

## Scenario 1

<br>
<br>

**A computational neuroscientist reruns a published fMRI analysis using the original dataset and Python scripts to verify the reported brain activation patterns.**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Reproducibility
:::

---

## Scenario 2

<br>
<br>

**An environmental scientist repeats a field experiment on soil nutrient levels using the same sampling protocol at a different site.**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Replicability
:::

---

## Scenario 3

<br>
<br>

**A linguist reanalyzes a corpus of historical texts using the same annotation guidelines and code to verify reported patterns of syntactic structures.**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Reproducibility
:::

---

## Scenario 4

<br>
<br>

**A psychology lab replicates a social behavior experiment using new participants from a different cultural background.**


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Replicability
:::

---

# Interactive slide

---

<section>
  <h2 style="font-size:1.5em;">Practical Exercise 1</h2>
  <p style="font-size:0.85em;">Decide whether each scenario is an example of <strong>reproducibility</strong> or <strong>replicability</strong>.</p>
  <ol style="font-size:0.8em;">
    <li class="fragment">
      A computational neuroscientist reruns a published fMRI analysis using the original dataset and Python scripts to verify the reported brain activation patterns.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Reproducibility</div>
    </li>
    <li class="fragment">
      An environmental scientist repeats a field experiment on soil nutrient levels using the same sampling protocol at a different site.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Replicability</div>
    </li>
    <li class="fragment">
      A psychology lab replicates a social behavior experiment using new participants from a different cultural background.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Replicability</div>
    </li>
    <li class="fragment">
      A linguist reanalyzes a corpus of historical texts using the same annotation guidelines and code to verify reported patterns of syntactic structures.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Reproducibility</div>
    </li>
  </ol>
</section>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Replicability
:::

---

## What published research can be replicated/reproduced?

| Field                          | Success | Failure |
|--------------------------------|---------|---------|
| OSC (2015) – Psychology         | 36%     | 64%     |
| Chang & Li (2015) – Economics (67 papers, 29 papers replicated)        | 43%     | 57%     |
| Camerer 2016 – Econ laboratory        | 61%     | 39%     |
| Camerer combined Social Sci     | 62%     | 38%     |
| Begley & Ellis (2012) – Cancer Research | 11%     | 89%     |
| Prinz et al. (2011) – Pharmaceutical research      | 35%     | 65%     |
| Cova et al. (2018) – x-philosophy       | 70%     | 30%     |
| Protzko et al. (2023) – Social   | 86%     | 14%     |


---

## The Reproducibility Project (2015)

- Large-scale replication project:
  - 100 studies from 3 different journals
  - Close/exact replications
  - Contacted original study authors
  - Open materials and data


::: {.callout-important}
**How many studies do you think replicated?**
:::

<div style="font-size: 0.3em; color: #777;">
Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.
</div>


::: {.notes}
**Speaker Notes**: The Reproducibility Project by the Open Science Collaboration (2015) involved a large-scale, collaborative effort by over 270 researchers. They selected 100 experimental studies published between 2008 and 2012 in three top psychology journals: Psychological Science, Journal of Personality and Social Psychology, and Journal of Experimental Psychology: Learning, Memory, and Cognition.
For each study, they carefully reviewed the original materials, methods, and analyses, and often contacted the original authors for clarification or access to materials and protocols. Each replication aimed to match the original study’s sample size, experimental design, and analysis plan as closely as possible, while making only minimal changes when exact materials were unavailable. The replications were preregistered to specify hypotheses, design, and analysis before data collection, reducing bias and “researcher degrees of freedom." Data collection was conducted independently from the original research teams, often in multiple labs to ensure rigor. After data collection, they analyzed the results using the same statistical methods as the original studies to determine whether the findings could be reproduced. The project also evaluated effect sizes, not just statistical significance, to compare the magnitude of the original and replicated effects.

**Only 39% of the replications produced statistically significant results, and the median effect size in replications was about half that of the original studies, highlighting the reproducibility challenges in psychology**.

**Instructor  Notes**:
:::
---

## Replicability and reproducibility across disciplines

```{r, echo = F}
library(ggplot2)
library(tidyverse)

# Sample data
df <- data.frame(Category = rep(c("Psychology", 
                                  "Cancer Research", 
                                  "Pharmaceutical Research",
                                  "Economics", 
                                  "Experimental Economics",
                                  "Experimental Philosophy"), 
                                each = 2),
                 Result = rep(c("successfully replicated",
                                "unsuccessfully replicated"), 
                              times = 6),
  Value = c(36, 64, 11, 89, 35, 65, 43, 57, 61, 39, 70, 30)
)

# Preserve order of bars on x-axis
df$Category <- factor(
  df$Category,
  levels = c("Psychology", 
             "Cancer Research", 
             "Pharmaceutical Research", 
             "Economics", 
             "Experimental Economics", 
             "Experimental Philosophy")
)

# Make stacking order explicit by setting factor levels for Series.
df$Series <- factor(df$Result, 
                    levels = c("successfully replicated",
                               "unsuccessfully replicated"))

# Plot
ggplot(df, aes(x = Category, y = Value, fill = Result)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(Value, "%")), # use position_stack(vjust = 0.5) to center a label inside each segment
            position = position_stack(vjust = 0.5),
            color = "white",
            size = 5) +
  labs(title = "",
       x = "",
       y = "Percentage (%)") +
  scale_fill_manual(values = c("successfully replicated" = "darkblue",
                               "unsuccessfully replicated" = "darkred"),
                    labels = function(x) str_wrap(x, width = 12)) + # wrap legend names
  theme_minimal() +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 12)) + # to wrap text
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),
        axis.text.y = element_text(size = 14),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        legend.text = element_text(size = 14))


```


<div style="font-size: 0.3em; color: #777;">
Open Science Collaboration (2015); Social Science: Combined sample of systematically sampled projects (RPP, SSRP, EERP); Chang & Li (2015); Camerer et al (2016); Begley, C. G., & Ellis, L. M. (2012). Prinz, F., Schlange, T., & Asadullah, K. (2011); Cova et al. (2018); Protzko et al., 2023)
</div>

---


## Which studies replicate (in psychology)?

| Field                          | Description | Link |
|--------------------------------|---------|---------|
| Psychology| Replication database | [https://forrt-replications.shinyapps.io/fred_explorer/](https://forrt-replications.shinyapps.io/fred_explorer/)    |

---


## Not a recent issue

<img src="images/reproducibility_literature.png" alt="An image with te titles of journal articles reporting about the replicability and reproducibility crisis across difference disciplines." style="display:block; margin:0 auto; width:50%; height:50%;">


<div style="font-size: 0.3em; color: #777;">
[Adapted from Dr. Malika Ihlehttps: https://osf.io/u3znx](https://osf.io/u3znx)
</div>

---


## A romantisiced idea of research ?

<img src="images/this-is-research.png" alt="An image describing a romantic view of research: The smartest heads in the world immerse themselves into a research topic for years.In that process, they become the experts – nobody knows more about that topic.The boundaries of knowledge have been pushed forward. When the researchers are confident in their findings, they publish them in the best scientific journals, with the highest standards of quality, rigor, and integrity." style="display:block; margin:0 auto; width:50%; height:50%;">

---

# Why did not more studies replicate?

---


## Published work is important for ...

- Getting a job
- Being awarded grants
- Being visible in the respective research field


::: {.callout-warning}
The consequence is a "rat race" culture: Researchers try to publish as much as we can.
:::

Ideal scenario: Balancing the desire to stay truthful to research with the necessity to publish?

---

## ...right?

![](images/07_holy_grail.png){fig-align=center width=50% fig-alt="chasing significance"}

---

## Relevance of publishing for career

| Actual (not desired) relevance in professorship hiring committees
       | Rank |
|-------------|------|
| **Number** of peer-reviewed publications | 1 |
| Fit of research profile to the hiring department | 2 |
| Quality of research talks | 3 |
| **Number** of publications | 4 |
| Volume of acquired third party funding | 5 |
| **Number** of first authorships | 6 |
| ... | ... |

N = 1453 psychology researchers, 66% were actually members of a professorship hiring committee.

<br> 

<div style="font-size: 0.3em; color: #777;">
Abele-Brehm, A. E., & Bühner, M. (2016). Wer soll die Professur bekommen? Psychologische Rundschau, 67(4), 250–261. http://doi.org/10.1026/0033-3042/a000335
</div>

---

## How do I get lots of publications?

![](images/02_significant_results_discipline_92.png){fig-align=center width=100% fig-alt="significant results"}

Fanelli, D. (2010). “Positive” Results Increase Down the Hierarchy of the Sciences. PLOS ONE, 5, e10068. doi:10.1371/journal.pone.0010068

---

## Publication bias

**"If it works, I can publish it; if it does not, let me put it into my file drawer."**

```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/file-drawer.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">
**Publication bias** happens when studies with positive or significant results are much more likely to be published than studies with negative or non-significant results.
  </div>

</div>

---

## Example publication bias

Turner et al 2008


---

## More than publication bias

De vries et al study

---


## Cognitive biases


---


## Confirmation bias


---


## Accidental p-hacking

**"P-hack... What now?"**

![](images/04_QRPs.png){fig-align=center width=100% fig-alt="p-hacking and questionble research practises"}

---

## Null-Hypothesis-Testing

**Example**: Does this new drug work better to decrease flu-symptoms compared to an existing drug?

- The null hypothesis (H₀) is the idea that nothing special is happening—for example, “the new drug is no better than the existing one.”
- You then collect data from experiments and perform a statistical analysis to see if the evidence is strong enough to reject the null hypothesis.
- The **p-value** tells you how likely it is to see the results you got **if the null hypothesis is actually true**.
- A p-value of 0.05 means: “There’s a 5% chance of seeing these results (or more extreme) if the null hypothesis is true.”


::: {.callout-important}
A p-value of 0.05 means that we accept a 5% chance that our results came about by pure luck (if the results were accidental, we would speak of a false positive result)
:::

---

# Increasing your false positive rates can happen in the blink of an eye!

---

## Have you ever ...?

- Neglected to blind data collection?
- Added more outcome variables in the analysis process and reported only the outcome variables that produced a significant effect?
- Ran many different comparisons on different subsets/groups etc.?
- Stopped data collection when you obtained the result you were looking for?
- Continued sampling after finding a null result?
- Reformulated your hypotheses based on what you found?
- Excluded outliers based on the significance of your results
- Tested excluding, including, or transforming covariates, but only reported the final model?
- Neglected to report all the dependent measures you tested?

---

## Tip 1: Report only outcome variables you "like" and/or silently add other outcomes

- For **two outcome variables**: False positive rate increases from 5% to **9.5%**

- For **five outcome variables**: False positive rate increases from 5% to **41%**


![](images/05_Outcome_Switching.png){fig-align=center width=100% fig-alt="p-hacking and questionble research practises"}

---

## Tip 1 (cont.): Report only outcome variables you like and/or silently add other outcomes


![](images/06_Silent_Outcomes.png){fig-align=center width=100% fig-alt="p hacking and questionable research practises"}

```text
http://compare-trials.org/  http://blogs.discovermagazine.com/neuroskeptic/2015/07/23/social-priming-money-for-nothing/#.VuKRSRi5KJM
```

---

## Tip 2: Experiment with many conditions, but report only those that worked


```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/06_Reporting_Conditions.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">

  - Transforming a thesis into a ground-breaking publication
  - The „Chrysalis effect“ (O’Boyle et al, 2017): 142 dissertations that were subsequently published in a refereed journal (149 field studies and 26 experiments)


  </div>

</div>



O’Boyle, E. H., Banks, G. C., & Gonzalez-Mulé, E. (2017). The Chrysalis Effect: How Ugly Initial Results Metamorphosize Into Beautiful Articles. Journal of Management, 43(2), 376–399. https://doi.org/10.1177/0149206314527133
https://twitter.com/JoeHilgard/status/699693258386051072

---

## Tip 3: Stop collecting data whenever you feel like it

- Collect an initial sample, analyze the results, add additional participants if not significant, stop when significance is found

- Increase twice: α = 11%

- But with enough looks can be pushed to 100%!



---

## Tip 4: Drop participants that you did not like

- Selective exclusion of data/outliers after seeing the results until the results are satisfying (aka until significance has been reached)

![](images/08_Selective_Exclusion.png){fig-align=center width=50% fig-alt="selective exclusion"}

---

## Tip 5: HARK-ing (personal favourite)

- **H**ypothesizing **A**fter **R**esults are **K**nown = presenting an exploratory finding to match a hypothesis that was created only after analysing the results

---

## Tip 6: Run as many different comparisons as humanly possible

- Run many different comparisons on different outcomes, subgroups, time windows, etc., and only report the comparison that produced p < 0.05

SPEAKER NOTES: Each time you try another independent test at α = 0.05 you have a 5% chance of a Type I error for that test. If you perform many uncorrected tests, the chance that at least one will be (falsely) significant rises quickly. For example, doing 20 independent tests at α = 0.05 gives a probability of at least one false positive of
1 − (1 − 0.05)²⁰ = 1 − 0.95²⁰ ≈ 0.6415 (≈64%).



---



## Human errors


---

## Statistical errors


---

## AI and the promotion of untrustworthy research 



---
















---

## Quality, rigor and integrity

*The smartest heads in the world immerse themselves into a research topic for years. In that process, they become the experts – nobody knows more about that topic. 
The boundaries of knowledge have been pushed forward. When the scientist are confident in their findings, they publish them in the best scientific journals, with the highest standards of quality, rigor, and integrity.*

::: {.callout-important}
How much of that literature is true do you think?
:::


::: {.notes}
**Speaker Notes**: Script for the slide here. 

**Instructor Notes**: Add. 
:::

---

## Why should we trust researchers?

![](images/01_snakesman_researcher.png){fig-align=center width=100% fig-alt="snakesman analogy"}

::: {.notes}
**Speaker notes**: Add script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: Core theoretical introduction of submodule topic.
- For a 90-minute lesson, the instructor should try to "lecture" for only 20 minutes, students should work in groups/pairs/on their own for at least 55 minutes of the lesson (+ a 15 minute break).
- Pair theoretical aspects with practical exercises and group discussions according to the Think-Pair-Share style and according to Cognitive Load Theory (Sweller, 1980).
- Use multiple slides for this part.
:::

---

## What research is all about

Text

---

# How to be successful in academia (or: How to hack your way to scientific glory)


---

## Tips on how to p-hack (like the pros)

---