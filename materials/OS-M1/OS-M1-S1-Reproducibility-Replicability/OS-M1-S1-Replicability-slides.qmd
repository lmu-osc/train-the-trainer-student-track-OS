---
title: "Threats to credible research: How Open Research can change the game"
author: "Sarah von Grebmer zu Wolfsthurn"
date: "today"
date-format: "DD/MM/YYYY"
format: 
  revealjs:
    css: ../../../slides-custom.css # looks for css file in root
    footer: LMU Open Science Center
    slide-number: true
    logo: ../../../OSC_FORRT_Logo.png  # Inserts logo in the bottom right corner (default)
  pdf:
    geometry: 
      - top=30mm
      - left=30mm
  pptx: 
    css: ../../../slides-custom.css # looks for css file in root

bibliography: ../../../assets/references.bib
csl: ../../../assets/apa.csl

execute:
  echo: true
  eval: true
  engine: knitr
---

## Licence

<br>

<p style="text-align:center;">
  <img src="https://licensebuttons.net/l/by/4.0/88x31.png"
       alt="CC BY 4.0"
       style="height:50px;">
</p>

<div style="background-color: #f0f0f0; padding: 0.05em; border-radius: 2px; font-size: 0.6em;">
This work was originally created by [Felix Schoenbrodt](https://www.nicebread.de/) under a CC-BY 4.0 [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en). This current work by Sarah von Grebmer zu Wolfsthurn, Malika Ihle and Felix Schoenbrodt is licensed under a CC-BY 4.0 [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/deed.en). It permits unrestricted re-use, distribution, and reproduction in any medium, provided the original work is properly cited.
</div>

::: {.notes}
**Speaker Notes**: The Creative Commons Attribution 4.0 International (CC BY 4.0) license permits you to freely use, share, and adapt the licensed material for any purpose, including commercial use. This means you can copy, redistribute, remix, transform, and build upon the work without asking for permission.
However, there are some important conditions you must follow. You are required to give appropriate credit to the original creator, provide a link to the license, and indicate if you made any changes to the material. Additionally, you cannot apply any legal terms or technological measures that would restrict others from using the work under the same freedoms.
:::

---

## Contribution statement

<br>

**Creator**: Von Grebmer zu Wolfsthurn, Sarah (![ORCID Logo](https://orcid.org/sites/default/files/images/orcid_16x16.png)[ 0000-0002-6413-3895](https://orcid.org/0000-0002-6413-3895))

**Reviewer**: Ihle, Malika (![ORCID Logo](https://orcid.org/sites/default/files/images/orcid_16x16.png)[0000-0002-3242-5981](https://orcid.org/0000-0002-3242-5981))

**Consultant**: Schönbrodt, Felix (![ORCID Logo](https://orcid.org/sites/default/files/images/orcid_16x16.png)[0000-0002-8282-3910](https://orcid.org/0000-0002-8282-3910))

::: {.notes}
**Speaker Notes**: These are the **speaker notes**. You will a script for the presenter for every slide. In presentation mode, your audience will not be able to see these speaker notes, they are only visible to the presenter. 

**Instructor Notes**: There are also **instructor notes**. For some slides, there will be pedagogical tips, suggestons for acitivities and troubleshooting tips for issues your audience might run into. You can find these notes underneath the speaker notes.

**Acessibility Tips**: Where applicable, this is a space to add any tips you may have to facilitate the accessibility of your slides and activities. 
:::

---

## Prerequisites

::: {.callout-important}
Before completing this submodule, please carefully read about the prerequisites.
:::


<div style="font-size: 0.8em;">
| Prerequisite   |  Description  | Link/Where to find it   |
|------------|------------|------------|
| UNESCO Recommendations on Open Science | Recommended reading: pp 6-19 | [Download here](https://unesdoc.unesco.org/ark:/48223/pf0000379949) |
</div>

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: Since this is the introductory submodule of this module, there are no formal prerequisites. However, to get a small kick-start into the topic, learners are adviced to have skimmed the UNESCO Recommendations on Open Science, particularly pages 6-19. 
:::

---

## Before we start - survey time

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: The pre-submodule survey serves to examine students' prior knowledge about the submodule's topic.
- Use free survey software such as particify or formR to establish the following questions (shown on separate slides).
:::

---

**Based on your experience so far, how would you currently rate your trust in published scientific findings on a scale from 1 - 5? (1 = not trusting any of the findings, 2 = trusting only some findings, 3 = trusting about half of the findings, 4 = trusting the majority of the findings,  5 =  trusting all findings)**

a. 1

b. 2

c. 3

d. 4

e. 5

---

**Based on your experience so far, do you currently see any challenges in research?**

Wordcloud answer. 

---

**Based on your experience so far, which concepts to you connect to research more broadly?**

Wordcloud answer. 

---

**What is your level of familiarity with Open Research practices in general (e.g., basic concepts, terminology, or tools)?**

a. I am unfamiliar with the concept of Open Research practices.

b. I have heard of them but I would not know how they apply to my work.

c. I have basic understanding and experience with Open Research practices in my own work/research/studies. 

d. I am very familiar with Open Research practices and routinely apply them in my daily work/research/study routines.

---

## Discussion of survey results

<br>

<div style="background-color: #f0f0f0; padding: 0.1em; border-radius: 5px; font-size: 1em; text-align: center;">

What do we see in the results?

</div>

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim"**: Briefly examine the answers given to each question interactively with the group.
- Use visuals from the survey to highlight specific answers.
:::

---

## Where are we at?

**Previously**:

- Point 1
- Point 2

<div style="background-color: #f0f0f0;">
**Up next**:

- Point 1
- Point 2
</div>


::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: Place the topic of the current submodule within a broader context.
- Remind students what you are working towards and what the bigger picture is.
:::

---

## Covered in this session

- **Aim**: This slides serves as an overview of the topics that are discussed, presented as bullet point:
- Topic 1
- Topic 2
- Topic 3

::: {.notes}
**Speaker Notes**: Script for the slide here.
**Instructor Notes**: Add.
:::

---

## Learning goals

- **Aim**: Formulate specific, action-oriented goals learning goals which are measurable and observable in line with Bloom's taxonomy  [@andersonTaxonomyLearningTeaching2001; @bloom1955NormativeStudy1956]

- Place an emphasis on the **verbs** of the learning goals and choose verbs that align with the skills you want to develop or assess.
- Examples: 
  - Students will **describe** the process of photosynthesis or
  - Students will **construct** a diagram illustrating the process of photosynthesis
  
::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: Formulate specific, action-oriented goals learning goals which are measurable and observable in line with Bloom's taxonomy (Anderson et al., 2001; Bloom et al., 1956)
- Place an emphasis on the **verbs** of the learning goals and choose verbs that align with the skills you want to develop or assess.
- Examples: 
  - Students will **describe** the process of photosynthesis or
  - Students will **construct** a diagram illustrating the process of photosynthesis

:::

---

## Key terms and definitions

- **Aim**: Introduce key terms and definitions that students will come across throughout the session.
<br>

- **Key Term 1**: Definition
- **Key Term 2**: Definition
- **Key Term 3**: Definition

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Base yourself on conceptual change theory and examine existing concepts in relation to some key terms. Re-examine the formation of new concepts at the end of the lesson. 
:::

---

## Practical exercise 1

**What does "research" mean to you?**

::: {.notes}
**Speaker Notes**: Let's start from the beginning! What does research mean to you? What concepts to you connect with it? How would you describe it? Think about this for yourself for a few minutes, write down some notes. Then form pairs and discuss with your partner what they linked to this term. We will then share our ideas in the group.

**Instructor  Notes**: Use the think-pair-share method: Students write down for themselves for a minute what a definition of research could be, then pair with a neighbour to compare answers. Final answers are then discussed in the plenum. 
:::

---

## What is "research"?

<br>

<div style="background-color: #f0f0f0; font-size: 0.8em;">
"Research refers to a careful, well-defined (or redefined), objective, and **systematic method** of **search for knowledge**, or **formulation** of a theory that is driven by inquisitiveness for that which is unknown and useful on a particular aspect so as to make an original contribution to expand the existing knowledge base. Research involves the **formulation of hypothesis** or **proposition of solutions**, data analysis, and deductions; and ascertaining whether the conclusions fit the hypothesis. Research is a **process of creating, or formulating knowledge that does not yet exist.**"
</div>

<br>
<br>

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Deb, D., Dey, R., & Balas, V. E. (2018). Introduction: What is research? In *Engineering Research Methodology: A Practical Insight for Researchers* (pp. 1-7). Singapore: Springer Singapore.
</div>

::: {.notes}
**Speaker Notes**: As was also evident from our discussions from the previous exercise, research can take many forms, therefore finding an all-encompassing description can be tricky. The work by Deb and colleauges (2018) describes research in the following way: Research is a careful, well-defined, objective, and systematic process rather than a random or unstructured activity. It involves a deliberate search for knowledge or the formulation of theories that aim to explain aspects of reality that are not yet fully understood. This process is driven by inquisitiveness and a desire to explore what is unknown, unresolved, or potentially useful, with the ultimate goal of making an original contribution that expands the existing body of knowledge. Research does not simply repeat what is already known; instead, it seeks to refine, extend, or generate new understanding.

Central to the research process is the formulation of hypotheses or the proposition of possible solutions to identified problems. These guide the direction of inquiry and determine the type of data that must be collected. Through systematic data analysis and logical deduction, researchers evaluate evidence to determine whether their conclusions support or contradict the initial hypotheses. This evaluative step is essential, as it ensures that findings are grounded in evidence and coherent reasoning. Ultimately, research is a creative and constructive endeavor that results in the creation or formulation of knowledge that did not previously exist, thereby contributing meaningfully to scholarly and scientific advancement. 

**Instructor  Notes**: Add.
:::

---

## The research cycle

<img src="images/00_scientific_method.png" alt="A  diagram illustrating the scientific, with six stages arranged clockwise: Observation/Question, Research topic area, Hypothesis, Test with experiment, Analyze data, and Report Conclusions connected by arrows to show the ongoing, iterative research process." style="display:block; margin:0 auto; width:100%; height:100%;">


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
[www.phdcomics.com](www.phdcomics.com)
</div>


::: {.notes}
**Speaker Notes**: Let us walk through this diagram of the scientific process. This is basically a visual representation of what research can mean. As you can see, the stages are arranged in a clockwise cycle, emphasizing that science is continuous and iterative. We begin with Observation or the question. This is where something in the world prompts us to ask why or how it works, or we made an observation and wonder about a particular feature or consequence. Next, we move into researching the topic area. Here we gather existing knowledge, review what others have discovered, and refine our understanding of the problem. 

Based on what we learn, we develop a hypothesis. This is a testable explanation or prediction that we can investigate. Importantly, a testable hypothesis is a precise statement, derived from your original question. 

Then comes testing with an experiment. This is where we design procedures to measure, observe, and collect data in a controlled, systematic way.

After the experiment, we analyze the data. We look for patterns, compare results to our predictions, and determine whether the hypothesis holds up.

Finally, we report our fundings and conclusions, together with our approach. This could be through a written paper, a presentation, or sharing findings with a research team. Importantly, these conclusions often lead to new questions, sending us back to the first stage and continuing the cycle of discovery.

**Instructor  Notes**: Add.
:::

---

## The research cycle

<img src="images/00_research_cycle.png" alt="A circular diagram illustrating the design-based research cycle, with four stages arranged horizontally: Observation, formulation hypothesis, test hypothesis with experiment, establish theory based on repeated validation of results connected by arrows to show the ongoing, iterative research process." style="display:block; margin:0 auto; width:50%; height:50%;">


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
DBR English greyscaler (Design-based research cycle)” by Sarah Zloklikovits, licensed under CC BY 4.0 — Wikimedia Commons.
</div>

::: {.notes}
**Speaker Notes**: This is a similar diagram to the previous slide, but captures the cyclical nature of research much better. When we have written up our conclusions, we are likely left with more questions and additional observations we made in the process, leading to the start of a whole new research cycle. 
**Instructor  Notes**: Add.
:::

---

## Practical exercise 2

**Task**: For the following "observation", map out the individual steps of the research cycle: <br>

<br> 

**"I wonder what happens to my pasta when I cook it in unsalted water?"** 

::: {.notes}
**Speaker Notes**: I want you to think about these different stages and steps of the research cycle with reference to a concrete example. Let us imagine that you had the impression that on some days, your pasta cooks much faster compared to other days. You wonder why this is. You believe that the most likely explanation for this observation could be the quantity of salt. You formulate your observation in the following way: I wonder what happens to my pasta when I cook it in unsalted water? Your task is to map out the individual steps of a hypothetical approach where you try to answer this question in a systematic way. Use the research cycle as your framework of reference.

**Instructor  Notes**: A plausible working example:
  - Observation: I notice that pasta seems to cook faster when the water is salted. 
  
- Define the question: Does pasta cook faster in salted water?
  
- Research the topic area: What has been done before?
  
- Define hypothesis: Pasta will cook faster in salted water than in unsalted water. My outcome variable is cooking time in seconds.
  
- Test with experiment: Define the amount of pasta, amount of water, amount of salt, average heat and the pot to cook the pasta in. Measure cooking time with a watch. Repeat the experiment many times.
  
- Analyze data: Examine the data you collected and compare the cooking times between pasta cooked in salted vs. unsalted water. Define potential confounding factors. 
  
- Write up your statistical conclusion and answer your original question.
  
:::

---

# Can we trust in research?

---

## What is "trust" in research (general public perspective)?

::: incremental
- "*Society trusts that scientific research results are an honest and accurate reflection of a researcher’s work.*" <div style="display: flex; font-size: 0.55em; color: #777;">(Committee on Science, Engineering and Public Policy 2009: ix)</div>


- "*The public must be able to trust the science and scientific process informing public policy decisions.*" <div style="display: flex; font-size: 0.55em; color: #777;">(Obama 2009)</div>


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Resnik, D. B. (2011). Scientific research and the public trust. *Science and engineering ethics, 17*(3), 399-409.
</div>
:::

::: {.notes}
**Speaker Notes**: What do we mean by trust in research? In their paper, Resnik argues that maintaining public trust is essential for the scientific enterprise, and he highlights how U.S. policy leaders have emphasized this responsibility. For example, he references  the 2009 Committee on Science and Technology report, which highlights that the public expects science to be conducted responsibly and that preserving trust requires strong ethical norms, oversight, and systems that prevent misconduct. The part on "honest reflection of a researcher's work is particularly central here: society should be able to get a reliable and truthful insight into the methods and outcomes of the research process. They also cite President Obama’s 2009 statement on scientific integrity, which stresses that government science must be guided by facts, transparency, and freedom from political manipulation. Resnik uses this to illustrate that public trust in science and research is not automatic. Going even further, they argue that trust needs to be earned and protected, given that science and research play a critical role in informing policies relevant to the public. 


**Instructor  Notes**: Add.
:::

---

## What is "trust" in research?

::: incremental
- No consensus on definition from the perspective fo researchers

- Trust is **essential for effective collaboration** among researchers (includes co-authorship, peer review, data sharing, replication, teaching, mentoring etc.)

- Scientists reading published research trust that the **work was conducted as described**, that **all relevant methodological details** are **disclosed**, and that the data have **not been fabricated or falsified**


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Resnik, D. B. (2011). Scientific research and the public trust. *Science and engineering ethics, 17*(3), 399-409.
</div>

:::

::: {.notes}
**Speaker Notes**: When discussing the importance of trust in research by society, Resink highlights that trust is absolutely essential for effective collaboration in science. Researchers rely on one another in so many ways, e.g., co-authoring papers, reviewing each other’s work, sharing data, attempting replications, and even in day-to-day activities like teaching and mentoring. None of these collaborative practices would function smoothly without a basic level of confidence that everyone is acting with integrity.

Likewise, when scientists read published research, they trust that the study was carried out as the authors describe. They expect that all the important methodological details are reported transparently, so the work can be evaluated or repeated. And they assume that the data are genuine, or in other words that nothing has been fabricated, falsified, or manipulated; intentionally or unintentionally. Without this trust in the accuracy and honesty of the research record, the entire scientific research process becomes unstable and no longer credible.
**Instructor  Notes**: Add.
:::

---

## Practical exercise 3


As a researcher, what can *you* do to make your pasta experiment **trustworthy**?

::: {.callout-tip}
Think about your approach when formulating your hypothesis, when conducting your experiment, when analying your data, when writing up your findings; but also about potential confounding variables or hurdles you could encounter during the research process. 
:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

## Replicability and reproducibility

<div style="font-size: 0.8em;">
| Feature | **Replicability**| **Reproducibility** |
|------------|------------------|---------------------|
| Definition | Ability to **repeat an experiment** using the **same methods** and obtain the same results | Ability to **obtain consistent results** using the **original data and code** |
| Focus                 | *aka* repeating the experiment and collecting new data  | *aka* re-analyzing the original data with the original code etc.|
| Materials | Same experiment setup, protocols, conditions etc. | Original data, analysis scripts, code etc.|
| In practise  | Running the same psychological experiment with new participants | Running the published analysis on the original dataset |
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

## Replicability and reproducibility

::: {.callout-tip}
# Additional exercises

Want to practice how to distinguish the two? Skip to the end of the slides for additional exercises on replicability vs. reproducibility.
:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

## Replicability and reproducibility across disciplines

```{r, echo = F}
library(ggplot2)
library(tidyverse)

# Sample data
df <- data.frame(Category = rep(c("Psychology (n = 97)", 
                                  "Cancer Research (n = 53)", 
                                  "Pharmaceutical Research (n = 67)",
                                  "Economics (n = 67)", 
                                  "Experimental Economics (n = 16)",
                                  "Experimental Philosophy (n = 40)"), 
                                each = 2),
                 Result = rep(c("successfully replicated",
                                "unsuccessfully replicated"), 
                              times = 6),
  Value = c(36, 64, 11, 89, 35, 65, 43, 57, 61, 39, 70, 30)
)

# Preserve order of bars on x-axis
df$Category <- factor(
  df$Category,
  levels = c("Psychology (n = 97)", 
             "Cancer Research (n = 53)", 
             "Pharmaceutical Research (n = 67)",
             "Economics (n = 67)", 
             "Experimental Economics (n = 16)",
             "Experimental Philosophy (n = 40)")
)

# Make stacking order explicit by setting factor levels for Series.
df$Series <- factor(df$Result, 
                    levels = c("successfully replicated",
                               "unsuccessfully replicated"))

# Plot
ggplot(df, aes(x = Category, y = Value, fill = Result)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(Value, "%")), # use position_stack(vjust = 0.5) to center a label inside each segment
            position = position_stack(vjust = 0.5),
            color = "white",
            size = 5) +
  labs(title = "",
       x = "",
       y = "Percentage (%)") +
  scale_fill_manual(values = c("successfully replicated" = "darkblue",
                               "unsuccessfully replicated" = "darkred"),
                    labels = function(x) str_wrap(x, width = 12)) + # wrap legend names
  theme_minimal() +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 10)) + # to wrap text
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 14),
        axis.text.y = element_text(size = 14),
        axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14),
        legend.text = element_text(size = 14))


```


<div style="font-size: 0.55em; color: #777;">
Begley, C. G., & Ellis, L. M. (2012); Camerer et al (2016); Chang & Li (2015); Cova et al. (2018); Open Science Collaboration (2015); Social Science: Combined sample of systematically sampled projects (RPP, SSRP, EERP); Prinz, F., Schlange, T., & Asadullah, K. (2011); Protzko et al. (2023)
</div>

::: {.notes}
**Speaker Notes**: Different studies looked at the replicability of findings. They provide us with interesting and probably alarming results about whether independent researchers can repeat the study using new data and get the same overall findings or conclusions. 

As you can see here, the dark red bars are the % of studies which were not successfully replicated. Psychology is quite up there, about 64% of all studies we conduct do not replicate. What is equally as worrying, if not more, are that 89% of studies from cancer research do not replicate either. The best field really is  experimental philosophy, so from this, they have the most robust and credible scientific approach. 

A lot of aspects of these studies can be discussed - also critically discussed. For example: Were the studies selected randomly (or did they only select the studies that looked fishy from the start?). How do you even measure the „replicability“? Is simply looking for significance in the replications study sufficient or even useful? All of these are good points, and there is a whole emerging field that tackles these questions. It is called meta-science, and it takes a scientific view onto the scientific enterprise itself.

Protzko et al. (2023): Original studies were preregistered, used large samples, and shared all research materials. Protzko and colleagues implemented these behaviors in a best-practice prospective replication study in which four independent laboratories replicated novel findings in a round-robin format. Suggesting that high replicability is achievable. This study does not, however, provide causal evidence of specific practices that increase replicability.

**Instructor  Notes**: Add. 
:::

---

## Psychology: The Reproducibility Project (2015)

:::incremental

- Large-scale replication project:
  - Close/exact replications of 100 experimental and correlational studies from 3 different psychological journals
  - Reproducibility evaluated based on effect sizes, p-values, subjective assessment of replication teams
  - Contacted original study authors when necessary

:::


::: {.callout-important}
**What did they find?**

*Large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors.*
:::

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. *Science, 349*(6251), aac4716.
</div>

::: {.notes}
**Speaker Notes**: The Reproducibility Project by the Open Science Collaboration (2015) involved a large-scale, collaborative effort by over 270 researchers. They selected 100 experimental studies published between 2008 and 2012 in three top psychology journals: Psychological Science, Journal of Personality and Social Psychology, and Journal of Experimental Psychology: Learning, Memory, and Cognition.
For each study, they carefully reviewed the original materials, methods, and analyses, and often contacted the original authors for clarification or access to materials and protocols. Each replication aimed to match the original study’s sample size, experimental design, and analysis plan as closely as possible, while making only minimal changes when exact materials were unavailable. The replications were preregistered to specify hypotheses, design, and analysis before data collection, reducing bias and “researcher degrees of freedom." Data collection was conducted independently from the original research teams, often in multiple labs to ensure rigor. After data collection, they analyzed the results using the same statistical methods as the original studies to determine whether the findings could be reproduced. The project also evaluated effect sizes, not just statistical significance, to compare the magnitude of the original and replicated effects. **Only 39% of the replications produced statistically significant results, and the median effect size in replications was about half that of the original studies, highlighting the reproducibility challenges in psychology**.

**Instructor  Notes**:
:::

---

## Not a recent issue

<img src="images/reproducibility_literature.png" alt="An image with te titles of journal articles reporting about the replicability and reproducibility crisis across difference disciplines." style="display:block; margin:0 auto; width:100%; height:100%;">


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Adapted from Dr. Malika Ihle: [https: https://osf.io/u3znx](https://osf.io/u3znx)
</div>


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## Why did not more studies replicate?

<style>
/* Scope all styles to this slide only */
.bubble-slide {
  position: relative;
  min-height: 100%;
}

/* Bubble styling */
.bubble-slide .bubble {
  position: absolute;
  padding: 18px 26px;
  border-radius: 999px;
  color: white;
  font-weight: 700;
  font-size: 1.1em;
  text-align: center;
  white-space: nowrap;
  box-shadow: 0 4px 10px rgba(0,0,0,0.25);
}

/* Colors */
.bubble-slide .red    { background-color: #e74c3c; }
.bubble-slide .blue   { background-color: #3498db; }
.bubble-slide .green  { background-color: #2ecc71; }
.bubble-slide .purple { background-color: #9b59b6; }
.bubble-slide .orange { background-color: #e67e22; }
.bubble-slide .gray   { background-color: #808080; }
</style>

<div class="bubble-slide">
  <div class="bubble red" style="top: 10%; left: 10%;">human bias</div>
  <div class="bubble blue" style="top: 25%; left: 60%;">lack of resources</div>
  <div class="bubble green" style="top: 45%; left: 20%;">methodological flaws</div>
  <div class="bubble purple" style="top: 50%; left: 65%;">pressure</div>
  <div class="bubble orange" style="top: 20%; left: 40%;">??</div>
  <div class="bubble gray" style="top: 65%; left: 5%;">lack of statistical knowledge</div>
</div>

---

## Why published papers are useful

::: incremental
- Getting a **job**

- Being awarded **grants**

- Being **visible** in the respective research field

- ...
:::

---

## Why published papers are useful

- Getting a **job**

- Being awarded **grants**

- Being **visible** in the respective research field

- ...

::: {.callout-warning}
The consequence can be a "*rat race*" culture: Researchers try to publish as much as they can [@schmidtCreatingSPACEEvolve2021]
:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## Career relevance of publishing

- Survey among N = 1453 psychology researchers, 66% were actually members of a professorship hiring committee

<div style="font-size: 0.7em;">
| **Actual (not desired) relevance in professorship hiring committees** | **Rank** |
|------------------------------------------------------------------|------|
| **Number** of peer-reviewed publications | 1 |
| Fit of research profile to the hiring department | 2 |
| Quality of research talks | 3 |
| **Number** of publications | 4 |
| Volume of acquired third party funding | 5 |
| **Number** of first authorships | 6 |
</div>


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Abele-Brehm, A. E., & Bühner, M. (2016). Wer soll die Professur bekommen? *Psychologische Rundschau, 67*(4), 250–261. [http://doi.org/10.1026/0033-3042/a000335](http://doi.org/10.1026/0033-3042/a000335)
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## How do I get lots of publications?

![](images/02_significant_results_discipline_92.png){fig-align=center width=100% fig-alt="significant results"}

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Fanelli, D. (2010). “Positive” Results Increase Down the Hierarchy of the Sciences. *PLOS ONE, 5*, e10068. [https://doi.org/10.1371/journal.pone.0010068](https://doi.org/10.1371/journal.pone.0010068)
</div>

---

## Publication bias

"If my study *works*, I can publish it. *If it does not, let's hide it the drawer.*"

```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/file-drawer.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">
**Publication bias** happens when studies with *positive* or *significant* results are much more likely to be published than studies with negative or non-significant results.
  </div>

</div>

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Song, F., Hooper, L., & Loke, Y. K. (2013). Publication bias: what is it? How do we measure it? How do we avoid it?. *Open Access Journal of Clinical Trials*, 71-81. [https://doi.org/10.2147/OAJCT.S34419](https://doi.org/10.2147/OAJCT.S34419)
</div>

---

## Example publication bias

![](images/Turner_Study_Publication_Bias.png){fig-align=center width=100% fig-alt="Turner study"}

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Turner, E. H., Matthews, A. M., Linardatos, E., Tell, R. A., & Rosenthal, R. (2008). Selective publication of antidepressant trials and its influence on apparent efficacy. *New England Journal of Medicine, 358*(3), 252-260. [https://doi.org/10.1056/NEJMsa065779](https://doi.org/10.1056/NEJMsa065779)
</div>

::: {.notes}
**Speaker Notes**: In this study, the authors obtained the archive of clinical trials for 12 antidepressant drugs submitted the the US Food and Drug Administration (FDA). In total, the trials involved more than 12000 patients. They performed a systematic search to identify which of these trials had been published in the literature. For those that were published, the compared the published versions and results with the versions and results in the FDA documentation. There were 74 FDA-registered trials. Of those, 38 trials showed postive effects of antidepressant drugs, 36 showed negative effects of antidepressant drugs. This is a 51% - 49% split of results: about half of the FDA-registered trials showed a positive effect of antidepressant drugs, the other half showed a negative effect of antidepressant drugs. 
37 of the 38 trials that showed positive effects were published in the literature, basically all of them except 1 trial. However, of the 36 trials that showed negative or no effects of antidepressant drugs, only 3 were published that included those exact findings. 22 of the 36 trials were not published at all, and the remaining 11 trials were published with the outcomes reframed in a more positive way. In the literature, in the end, we ended up with a total of 51 trials being published, 48 of which showed positive effects of antidepressant drugs on depression. That is 94% of this sample of papers. Only 6% (3 papers) showed a negative effect of antidepressant drugs on depression. As a consequence, those of us searching the literature for these papers, will get a distorted via of the effectiveness of antidepressants. Therefore, the authors were able to show a substantial publication bias in the antidepressant trial literature. 
**Instructor  Notes**: Add. 
:::

---

## More than just publication bias

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
de Vries, Y. A., Roest, A. M., de Jonge, P., Cuijpers, P., Munafò, M. R., & Bastiaansen, J. A. (2018). The cumulative effect of reporting and citation biases on the apparent efficacy of treatments: the case of depression. *Psychological Medicine, 48*(15), 2453–2455. [https://doi.org/10.1017/S0033291718001873](https://doi.org/10.1017/S0033291718001873)
</div>

::: incremental

```{=html}

<div style="display: flex; align-items: flex-start; gap: 0.5em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/12_Publication-bias.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1; font-size:0.8em;">

  (a) All conducted studies
  (b) **Study publication bias**: non-publication of an entire study
  (c) **Outcome reporting bias**: non-publication of negative outcomes within a published article or switching the status of (non-significant) primary and (significant) secondary outcomes
  (d) **Spin**: authors conclude that the treatment is effective despite non-significant results on the primary outcome
  (e) **Citation bias**: Studies with positive results receive more citations than negative studies


  </div>

</div>

:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## "Thought so already" bias

![](images/14_cognitive_bias.png){fig-align=center width=100% fig-alt="confirming your own beliefs"}

---

## "I knew it" bias

![](images/hindsight-bias.png){fig-align=center width=700% fig-alt="confirming your own beliefs"}

---

## Practical exercise 4

**Task**: Match the bias to its description.

<div style="font-size:0.6em;">

| **Bias** | **Description** |
|------------|-----------------|
| **1.** Confirmation bias | **A.** Non-publication of negative outcomes within a paper, or switching non-significant primary outcomes with significant secondary ones.  |
| **2.** Spin | **B.** Studies with positive results receive more citations than negative studies.  |
| **3.** Study publication bias | **C.** After learning the outcome, believing “I knew it all along.” |
| **4.** Hindsight bias | **D.** Non-publication of an entire study (e.g., trials with null results never submitted). |
| **5.** Outcome reporting bias | **E.** Tendency to seek or interpret information in ways that confirm existing beliefs. |
| **6.** Citation bias | **F.** Authors conclude the treatment is effective despite non-significant primary outcomes.|

</div>


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Solutions: 1E, 2F, 3D, 4C, 5A, 6B.  
:::

---


## Pre-break survey

- **Aim**: This pre-break survey serves to examine students' current understanding of key concepts of the submodule
- Use free survey software such as  or other survey software (particify, formR) to establish the following questions (shown on separate slides)

::: {.notes}
**Speaker Notes**: Script for the slide here.
**Instructor Notes**: <br>
- **Aim**: This pre-break survey serves to examine students' current understanding of key concepts of the submodule
- Use free survey software such as particify or formR to establish the following questions (shown on separate slides)
:::

---

**Which species is the largest type of penguin**?

a. Chinstrap Penguin

b. Emperor Penguin ✅

c. Adélie Penguin

d. King Penguin

---

**What is the key biological feature that helps penguins swim efficiently?**

a. Hollow bones for buoyancy

b. Webbed feet for paddling

c. Waterproof feathers and flipper-like wings ✅

d. Gills to breathe underwater

---

# Break! 15 minutes

---

## Post-break survey discussion

- **Aim**: To clarify concepts and aspects that are not yet understood
- Highlight specific answers given during the survey

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: To clarify concepts and aspects that are not yet understood
- Highlight specific answers given during the survey
:::

---

## Accidental *p*-hacking

**"P-hack... What now?"**

![](images/04_QRPs.png){fig-align=center width=100% fig-alt="p-hacking and questionble research practises"}

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Excursus: Null-Hypothesis-Significance Testing

**Example**: Does this new drug work better to decrease flu-symptoms compared to an existing drug?

::: incremental
- H₀ = “the new drug is not better than the existing one.”
- Collect data from experiments and perform a statistical analysis to see if the evidence is strong enough to reject H₀
- **P-value** = how likely it is to see the results you got **if H₀ is true**.
  - A p-value of 0.05 means: “*There’s a 5% chance of seeing these results (or more extreme) if the null hypothesis is true.*”
:::

::: {.callout-important}
A *p*-value of 0.05 means that we accept a 5% chance that our results came about by pure luck (if the results were accidental, we would speak of a false positive result)
:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Increasing your false positive rates can happen in the blink of an eye!

::: {.callout-caution}
Do **not** try the following at home.
:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Hack 1: Add *lots* of outcome variables

- For **two outcome variables**: False positive rate increases from 5% to **9.5%**

- For **five outcome variables**: False positive rate increases from 5% to **41%**


![](images/05_Outcome_Switching.png){fig-align=center width=100% fig-alt="p-hacking and questionble research practises"}

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Hack 2: Run as many comparisons as possible

```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/06_Reporting_Conditions.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">

  - Run as many different comparisons on **different outcomes, subgroups, time windows** etc. as you can
  - **Only** report the comparisons that produced a statistically significant result



  </div>

</div>

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
O’Boyle, E. H., Banks, G. C., & Gonzalez-Mulé, E. (2017). The Chrysalis Effect: How ugly initial results metamorphosize into beautiful articles. *Journal of Management, 43*(2), 376–399. [https://doi.org/10.1177/0149206314527133](https://doi.org/10.1177/0149206314527133)
</div>


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Hack 3: Stop collecting data whenever you found what you were looking for


```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/15_optional_stopping.png" style="width: 100%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">

  - Collect an initial sample, analyze the results, add participants if the results are not significant
  - **Stop** when significance is found
      - One analysis: α = 5%
      - Two analyses: α = 11%
      - But with enough “just looking” can be pushed to 100%!



  </div>

</div>

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Armitage, P., McPherson, C. K., & Rowe, B. C. (1969). Repeated significance tests on accumulating data. *Journal of the Royal Statistical Society. Series A (General), 132*, 235–244.
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Hack 4: Drop participants you do not like


```{=html}

<div style="display: flex; align-items: flex-start; gap: 2em;">

  <!-- Left side: single image -->
  <div style="flex: 1; text-align: center;">
    <img src="images/08_Selective_Exclusion.png" style="width: 70%;" />
  </div>
  
```

  <!-- Right side: bullet points -->
  <div style="flex: 1;">

  - Selectively **exclude data/ outliers** after seeing the results until the results are satisfying (*aka until significance has been reached*)




  </div>

</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Hack 5: HARK-ing

- **H**ypothesizing **A**fter **R**esults are **K**nown = presenting an exploratory finding to match a hypothesis that was created only after analysing the results

![](images/13_TexasSharpShooter.png){fig-align=center width=100% fig-alt="p-hacking and questionble research practises"}

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Combining these "hacks"

![](images/combining_hacks.png){fig-align=center width=100% fig-alt="p-hacking and questionble research practises"}

- Combining some of these hacks (aka questionable research practices can **raise false positive rates** from 5% to > 50%!
- The logic of the p-value is therefore **corrupted** and “*renders the reported p-values essentially uninterpretable.*”


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Stefan, A. M., & Schönbrodt, F. D. (2023). Big little lies: A compendium and simulation of p -hacking strategies. *Royal Society Open Science, 10*(2), 220346. [https://doi.org/10.1098/rsos.220346](https://doi.org/10.1098/rsos.220346)
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## A note to remember

::: {.callout-caution}
The so-called "hacks" on the past few slides represent **questionable research practices**. Do **not** try at home. 
:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Practical exercise 5

:::incremental

**Scenario**: You are reviewing a study examining whether drinking white tea improves short-term memory, where the researchers report:

  - Hypothesis: *White tea improves memory test scores.*
  
  - Sample size: 28 participants per group
  
  - Memory test score difference: *p* = 0.048
  
  - Results: 
    - Effect was “stronger in women” (*p* = 0.049)
    - Effect “even stronger when excluding two outliers” (*p* = 0.044)
    - No effect in men (*p* = 0.31)
    - Reaction time difference significant (*p* = 0.046)
  
  - Conclusion: *The results show that white tea reliably improves cognitive performance.*

**Which potential p-hacking strategies are at play here?**
:::

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Possible answers can include <br>

- Large number of uncorrected comparisons with selective reporting

- Many p values bunching just below 0.05

- Lack of pre registration or analysis plan when claims are confirmatory

- Results that disappear under slight analytic changes or when full data/code are provided.
:::

---

## Practical exercise 5

**Scenario**: You are reviewing a study examining whether drinking white tea improves short-term memory, where the researchers report:

- Hypothesis: *White tea improves memory test scores.*

- Sample size: 28 participants per group

- Memory test score difference: *p* = **0.048**

- Results: 
  - Effect was “stronger in **women**” (*p* = **0.049**)
  - Effect “even stronger when excluding two outliers” (*p* = **0.044**)
  - No effect in **men** (*p* = 0.31)
  - **Reaction time** difference significant (*p* = **0.046**)
  
- Conclusion: The results show that white tea reliably improves **cognitive performance**.

---

## The "real" scientific method

![](images/00_real_scientific_method.png){fig-align=center width=10% fig-alt="diagram of the real scientific method by phdcomics.com to illustrate the QRPs and biased nature inherent to the research cycle."}


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::


---

## Does this *actually* happen in real life?

Come on now. Surely not..? 

![](images/09_QRPs_Psychology.png){fig-align=center width=100% fig-alt="p-hacking and questionble research practises"}

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
John, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. *Psychological science, 23*(5), 524-532.
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

--- 

## .. and across fields?

- Survey among 6,813 academic researchers in The Netherlands: **Self-reported prevalence of fabrication and falsification in the last 3 years**


![](images/10_Self_Reported_blank.png){fig-align=center width=50% fig-alt="QRPs in psychology"}

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Gopalakrishna, G., ter Riet, G., Vink, G., Stoop, I., Wicherts, J. M., & Bouter, L. M. (2022). Prevalence of questionable research practices, research misconduct and their potential explanatory factors: A survey among academic researchers in The Netherlands. *PLOS ONE, 17*(2), e0263023. [https://doi.org/10.1371/journal.pone.0263023](https://doi.org/10.1371/journal.pone.0263023)
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## .. and across fields?

- Survey among 6,813 academic researchers in The Netherlands: **Self-reported prevalence of fabrication and falsification in the last 3 years**


![](images/11_Self_Reported_QRP.png){fig-align=center width=50% fig-alt="QRPs in psychology"}

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Gopalakrishna, G., ter Riet, G., Vink, G., Stoop, I., Wicherts, J. M., & Bouter, L. M. (2022). Prevalence of questionable research practices, research misconduct and their potential explanatory factors: A survey among academic researchers in The Netherlands. *PLOS ONE, 17*(2), e0263023. [https://doi.org/10.1371/journal.pone.0263023](https://doi.org/10.1371/journal.pone.0263023)
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## (Un)Intentional? 

- Intentional?
  - "Evil researcher" who only cares about his/her career and not at all about truth-seeking?
  - „*We urge the social science community to redefine p-hacking as a series of deceptive research practices rather than ones that are merely questionable.*“ [@craigUsingRetractedJournal2020]

- **Unintentional?**
  - Lack of education/knowledge?
  - Wrong/uncritical standards of the field?
  - Pushed by supervisors, reviewers, or editors? ➙ [http://bulliedintobadscience.org/](http://bulliedintobadscience.org/)
  - **Simply being human?**
  
::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.  
:::

---

## Human errors and honest mistakes

**"90% Excel-Gate"**

<style>
/* wrapper so styles don’t leak into other slides */
.overlap-slide {
  position: relative;
  width: 100%;
  height: 500px;   /* adjust as needed */
}

/* Image styling */
.overlap-slide img {
  position: absolute;
  width: 50%;      /* adjust size */
  border-radius: 8px;
  box-shadow: 0 6px 12px rgba(0,0,0,0.25);
}

/* slight rotation to make overlap nicer */
.overlap-slide .img3 { 
  top: 5%; 
  left: 5%; 
  transform: rotate(-3deg);
}

.overlap-slide .img2 { 
  top: 35%; 
  left: 1%; 
  transform: rotate(2deg);
}

.overlap-slide .img1 { 
  top: 2%; 
  left: 50%; 
  transform: rotate(-1deg);
}
</style>

<div class="overlap-slide">
  <img src="images/excel-gate.png" class="img1">
  <img src="images/excel-depression.png" class="img2">
  <img src="images/excel-mistake.png" class="img3">
</div>

::: {.notes}
**Speaker Notes**: In their 2010 paper Growth in a Time of Debt, economists Carmen Reinhart and Kenneth Rogoff claimed that when government debt exceeds 90% of GDP, economic growth falls sharply. Their findings strongly influenced debates about austerity policies. However, in 2013, Thomas Herndon, Michael Ash, and Robert Pollin from the University of Massachusetts–Amherst re-examined the data and found serious flaws: an Excel coding error that excluded several countries. In essence, what had happened was that when using a formula, the authors had made a mistake when selecting the range of cells where to apply the formula too. In this, they omitted some countries from the calculations. 

Once corrected, the average growth rate for high-debt countries rose from –0.1% to about +2.2%, showing no clear “90% tipping point.” The original authors later acknowledged the spreadsheet error and issued a correction.
**Instructor  Notes**: Add.  
:::

---

## Lessons learnt

::: {.callout-important}
The most important point of the story: The original authors **shared their raw data**, which made it possible to **correct** the honest mistake!
:::

::: {.notes}
**Speaker Notes**: The case became a landmark example of how data errors, selective reporting, and methodological choices can mislead economic policy, and of why transparency and replication are essential in research. Only because the data were available, the mistake could be found and corrected! 
**Instructor  Notes**: Add.  
:::

---

## Statistical errors

![](images/QuartelJournal_stat_analysis.png){fig-align=center width=100% fig-alt="QRPs in psychology"}

- Reproducible analysis **code and open data required** at submission - “inhouse checking” in review process

- 54% of all submissions had results in the paper that **did not match** the computed results from the code
  - Wrong signs, wrong labeling of regression coefficients, errors in sample sizes, wrong descriptive stats
  
<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Eubank, N. (2016). Lessons from a decade of replications at the quarterly journal of political science. *PS: Political Science & Politics, 49*(2), 273-276 [https://doi.org/10.1017/S1049096516000196](https://doi.org/10.1017/S1049096516000196)
</div>
  
::: {.notes}
**Speaker Notes**: This is an interesting paper which looked at code and data packages that were required at submission to the Quarterly Journal of Political Science. They took the analysis code that and the data that were submitted, reran the analysis and checked whether the results matched what was described in the paper. 

It finds that of the 24 empirical papers subjected to in-house replication review since September 2012, only four packages did not require any modifications. Most troubling, 14 packages (58%) had results in the paper that differed from those generated by the author’s own code. The authors identified a number of mistakes in the analysis scripts that were submitted and in the results that were published, for example that the wrong signs used for reporting, coefficients were labelled the wrong way, descriptive statistics were imprecise or incorrect.
Based on these experiences, this article presents a set of guidelines for authors and journals for improving the reliability and usability of replication packages.

**Instructor  Notes**: Add.  
:::


---

## Statistical inconsistencies

![](images/statcheck.png){fig-align=center width=10% fig-alt="QRPs in psychology"}

:::incremental
- 16,695 scanned papers with **statcheck tool** (Nuijten et al., 2015)
- **50%** of papers contain **statistical inconsistencies**
- 13% contained **strong errors** (i.e., where the statistical conclusion changes). 
- Numerical results of less than 30% of papers can be **reproduced** [@cruwellWhatsBadgeComputational2022]
:::

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Nuijten, M. B., Hartgerink, C. H., Van Assen, M. A., Epskamp, S., & Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). *Behavior research methods, 48*(4), 1205-1226. [https://doi.org/10.3758/s13428-015-0664-2](https://doi.org/10.3758/s13428-015-0664-2)
</div>

::: {.notes}
**Speaker Notes**: Add
**Instructor  Notes**: Add.  
:::

---

## What does this all mean?

<div style="background-color: #fff9c4; padding: 1em; border-radius: 10px; text-align: center; font-size: 1.2em;">
  **Bias + (accidental) p-hacking + human (honest) mistakes = untrustworthy research findings?**
</div>


::: {.callout-note}
- Published findings across fields to be viewed with **caution**?
- "We know" --> "We *think* we know"?
- More research to verify existing "truths"?
:::

::: {.notes}
**Speaker Notes**: Add
**Instructor  Notes**: Add.  
:::

---

## A romanticised idea of research?

<img src="images/this-is-research.png" alt="An image describing a romantic view of research: The smartest heads in the world immerse themselves into a research topic for years. In that process, they become the experts, nobody knows more about that topic.The boundaries of knowledge have been pushed forward. When the researchers are confident in their findings, they publish them in the best scientific journals, with the highest standards of quality, rigor, and integrity." style="display:block; margin:0 auto; width:100%; height:100%;">

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add. 
:::

---

## Why should we trust researchers?

![](images/01_snakesman_researcher.png){fig-align=center width=100% fig-alt="snakesman analogy"}

---

## Going even further: The perpetuating role of AI


<style>
/* wrapper so styles don’t leak into other slides */
.overlap-slide {
  position: relative;
  width: 100%;
  height: 500px;   /* adjust as needed */
}

/* Image styling */
.overlap-slide img {
  position: absolute;
  width: 50%;      /* adjust size */
  border-radius: 8px;
  box-shadow: 0 6px 12px rgba(0,0,0,0.25);
}

/* slight rotation to make overlap nicer */
.overlap-slide .img3 { 
  top: 1%; 
  left: 3%; 
  transform: rotate(-3deg);
}

.overlap-slide .img2 { 
  top: 35%; 
  left: 1%; 
  transform: rotate(2deg);
}

.overlap-slide .img1 { 
  top: 20%; 
  left: 45%; 
  transform: rotate(5deg);
}
</style>

<div class="overlap-slide">
  <img src="images/AI-advice.png" class="img1">
  <img src="images/AI-hallucinations.png" class="img2">
  <img src="images/overtrusting_AI.png" class="img3">
</div>

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
Emsley, R. (2023). ChatGPT: these are not hallucinations–they’re fabrications and falsifications. *Schizophrenia, 9*(1), 52. <br>
Lautrup, A. D., Hyrup, T., Schneider-Kamp, A., Dahl, M., Lindholt, J. S., & Schneider-Kamp, P. (2023). Heart-to-heart with ChatGPT: the impact of patients consulting AI for cardiovascular health advice. *Open Heart, 10*(2).
Shekar, S., Pataranutaporn, P., Sarabu, C., Cecchi, G. A., & Maes, P. (2025). People Overtrust AI-Generated Medical Advice despite Low Accuracy. *NEJM AI, 2*(6), AIoa2300015.
</div>

---

## Now what?

![](images/crossroads.png){fig-align=center width=100% fig-alt="crossroads"}

<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
This image was taken from the Geograph project collection. The copyright on this image is owned by Chris Martin and is licensed for reuse under the Creative Commons Attribution-ShareAlike 2.0 license.
</div>

---

## A new way of doing research

<br>

<p style="text-align: center; font-size: 1.5em;">
**Open Research** <br>
*aka* <br>
**A scientific framework for the 21. century**
</p>

---


# To be continued ...


---

## Where are we at?

:::incremental
**Today**:

- The research cycle
- Trust in research
- Replicability and reproducibility
- Challenges in research:
  - Biases
  - Questionble research practises/fabrication/falsificaton
  - Human errors and statistical errors
- **Towards a new approach to research**

<div style="background-color: #f0f0f0;">
**Up next**:

- **How Open Research can change the game** and *why*
</div>

:::

---

## Reflection activity

<br>
<br>

<div style="background-color: #f0f0f0;">
**One-minute paper**: Imagine you would have to explain the current challenges in research you heard about today to a friend. Write down what you would say to them.
</div>


::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: Add.
:::

---


## Take-home message

**What are you taking away from today?**

::: {.callout-tip}
## Remember: There are solutions!
Research is not "doomed" - on the contrary. More on this in the next session!
:::

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: Add.
:::

---

## To conclude: Survey time!

- **Aim**: This post-submodule survey serves to examine students' current knowledge about the sumodule's topic.
- Use free survey software such as  particify or formR to establish the following questions (shown on separate slides)
- Use identical questions as in the pre-submodule survey to be able to directly compare

::: {.notes}
**Speaker Notes**: Script for the slide here. 
**Instructor Notes**: <br>
- **Aim**: This post-submodule survey serves to examine students' current knowledge about the sumodule's topic.
- Use free survey software such as  particify or formR to establish the following questions (shown on separate slides)
- Use identical questions as in the pre-submodule survey to be able to directly compare
:::

---

**What is your level of familiarity with [Topic] (e.g., basic concepts, terminology, or tools)?**

a. I have never heard of it before.

b. I have heard of it but have never worked with it.

c. I have basic understanding and experience with it.

d. I am very familiar and have worked with it extensively.

---

**Which of the following concepts or skills do you feel most confident about in relation to [Topic]? (Select all that apply)**

a. Concept 1

b. Concept 2

c. Concept 3

d. Concept 4

e. I am not sure about any of these concepts.

---

**On a scale of 1 to 5, how comfortable are you with using [specific tool/technology] related to [Topic]? (1 = Not comfortable at all, 5 = Very comfortable)**

a. 1

b. 2

c. 3

d. 4

e. 5

---

## Discussion of survey results

- **Aim**: Briefly examine the answers given to each question interactively with the group.
- Compare and highlight specific differences in answers between pre- and post-survey answers

::: {.notes}
**Speaker Notes**: Script for the slide here.
**Instructer Notes**: <br>
- **Aim**: Briefly examine the answers given to each question interactively with the group.
- Compare and highlight specific differences in answers between pre- and post-survey answers
:::

---

# Thanks! <br>
See you next class :)

---

## Pedagogical add-on tools for instructors

- This section is dedicated to ideas on how to incorporate pedagogical tools into teaching for this specific submodule topic. This could mean:
  - Information about the scientific evidence on the theory of the pedagogical add-on tool and the evidence for its efficacy.
  - Discussion/reflection on how tools can be incorporated into the teaching for this particular content.
  - Extra exercises for faster students.	

---

## Additional literature for instructors

- References for content
- References for pedagogical add-on tools	
- Other resources (videos etc.)	

---

## ...right? MOVE

![](images/07_holy_grail.png){fig-align=center width=50% fig-alt="chasing significance"}

---

## Decide where to add

**Ideal scenario: Balancing the desire to stay truthful to research with the necessity to publish?**


---

## Additional exercises: Replicability and reproducibility

Decide whether each scenario in the following slides is an example of **reproducibility** or **replicability**. 

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Think pair share. 
:::

---

## Scenario 1

<br>
<br>

**A computational neuroscientist reruns a published fMRI analysis using the original dataset and Python scripts to verify the reported brain activation patterns.**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Reproducibility
:::

---

## Scenario 2

<br>
<br>

**An environmental scientist repeats a field experiment on soil nutrient levels using the same sampling protocol at a different site.**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Replicability
:::

---

## Scenario 3

<br>
<br>

**A linguist reanalyzes a corpus of historical texts using the same annotation guidelines and code to verify reported patterns of syntactic structures.**

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Reproducibility
:::

---

## Scenario 4

<br>
<br>

**A psychology lab replicates a social behavior experiment using new participants from a different cultural background.**


::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Replicability
:::

---

# Interactive slide

---

<section>
  <h2 style="font-size:1.5em;">Practical Exercise 1</h2>
  <p style="font-size:0.85em;">Decide whether each scenario is an example of <strong>reproducibility</strong> or <strong>replicability</strong>.</p>
  <ol style="font-size:0.8em;">
    <li class="fragment">
      A computational neuroscientist reruns a published fMRI analysis using the original dataset and Python scripts to verify the reported brain activation patterns.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Reproducibility</div>
    </li>
    <li class="fragment">
      An environmental scientist repeats a field experiment on soil nutrient levels using the same sampling protocol at a different site.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Replicability</div>
    </li>
    <li class="fragment">
      A psychology lab replicates a social behavior experiment using new participants from a different cultural background.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Replicability</div>
    </li>
    <li class="fragment">
      A linguist reanalyzes a corpus of historical texts using the same annotation guidelines and code to verify reported patterns of syntactic structures.
      <div style="color:#555; font-size:0.75em;"><strong>Answer:</strong> Reproducibility</div>
    </li>
  </ol>
</section>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Correct answer: Replicability
:::

---

## What published research can be replicated/reproduced? REMOVE

<div style="font-size: 0.5em;">
| Field                          | Success | Failure |
|--------------------------------|---------|---------|
| OSC (2015) – Psychology         | 36%     | 64%     |
| Chang & Li (2015) – Economics (67 papers, 29 papers replicated)        | 43%     | 57%     |
| Camerer 2016 – Econ laboratory        | 61%     | 39%     |
| Camerer combined Social Sci     | 62%     | 38%     |
| Begley & Ellis (2012) – Cancer Research | 11%     | 89%     |
| Prinz et al. (2011) – Pharmaceutical research      | 35%     | 65%     |
| Cova et al. (2018) – x-philosophy       | 70%     | 30%     |
| Protzko et al. (2023) – Social   | 86%     | 14%     |
</div>

---

## What does this mean for research? 

- Published findings across fields to be viewed with **caution**?
- "We know" --> "We *think* we know"?
- More research to verify existing "truths"?

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

## Psychology: The Replication Database by FORRT

- Collects replication results across different psychological fields
- Provide detailed overview of the original findings and the replication outcomes

![](images/FORRT_Replication_Database.png){fig-align=center width=100% fig-alt="An image of the landing page of the FORRT Replication database"}


<div style="display: flex; font-size: 0.55em; color: #777; justify-content: center;">
**FORRT Replication Database** ([https://forrt-replications.shinyapps.io/fred_explorer/](https://forrt-replications.shinyapps.io/fred_explorer/))
</div>

::: {.notes}
**Speaker Notes**: Add script for slide here. 
**Instructor  Notes**: Add.
:::

---

## Have you ever ...?

- Neglected to blind data collection?
- Added more outcome variables in the analysis process and reported only the outcome variables that produced a significant effect?
- Ran many different comparisons on different subsets/groups etc.?
- Stopped data collection when you obtained the result you were looking for?
- Continued sampling after finding a null result?
- Reformulated your hypotheses based on what you found?
- Excluded outliers based on the significance of your results
- Tested excluding, including, or transforming covariates, but only reported the final model?
- Neglected to report all the dependent measures you tested?

---

## References {.smaller}

::: {#refs}
:::

---
